{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URFkMTKK1iBs",
        "outputId": "31619644-e263-4c09-ed97-38b7dec18636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.63 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n",
            "Collecting bcolz-zipline\n",
            "  Downloading bcolz_zipline-1.2.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from bcolz-zipline) (1.26.4)\n",
            "Downloading bcolz_zipline-1.2.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bcolz-zipline\n",
            "Successfully installed bcolz-zipline-1.2.9\n",
            "Collecting insightface\n",
            "  Downloading insightface-0.7.3.tar.gz (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from insightface) (1.26.4)\n",
            "Collecting onnx (from insightface)\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from insightface) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from insightface) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from insightface) (3.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from insightface) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from insightface) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from insightface) (1.3.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from insightface) (0.23.2)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from insightface) (1.13)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from insightface) (3.0.10)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (from insightface) (1.4.12)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from insightface) (3.10.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (4.12.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (2.8.2)\n",
            "Requirement already satisfied: albucore>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (0.0.12)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (4.10.0.84)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (3.3)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (2024.7.24)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (2.8.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->insightface) (3.20.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->insightface) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->insightface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->insightface) (3.5.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from albucore>=0.0.11->albumentations->insightface) (2.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations->insightface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations->insightface) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->insightface) (1.16.0)\n",
            "Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: insightface\n",
            "  Building wheel for insightface (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for insightface: filename=insightface-0.7.3-cp310-cp310-linux_x86_64.whl size=1055334 sha256=cd434a20748b6b2177bfcbd9e10771b28277659ee8b2030a416719e2ff4774e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/d0/80/e3773fb8b6d1cca87ea1d33d9b1f20a223a6493c896da249b5\n",
            "Successfully built insightface\n",
            "Installing collected packages: onnx, insightface\n",
            "Successfully installed insightface-0.7.3 onnx-1.16.2\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting mtcnn\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (2.15.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.26.4)\n",
            "Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.1\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Collecting eel\n",
            "  Downloading eel-0.17.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bottle (from eel)\n",
            "  Downloading bottle-0.12.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting bottle-websocket (from eel)\n",
            "  Downloading bottle-websocket-0.2.9.tar.gz (2.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from eel) (1.0.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from eel) (3.1.2)\n",
            "Collecting whichcraft (from eel)\n",
            "  Downloading whichcraft-0.6.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting gevent-websocket (from bottle-websocket->eel)\n",
            "  Downloading gevent_websocket-0.10.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting gevent (from gevent-websocket->bottle-websocket->eel)\n",
            "  Downloading gevent-24.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting zope.event (from gevent->gevent-websocket->bottle-websocket->eel)\n",
            "  Downloading zope.event-5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting zope.interface (from gevent->gevent-websocket->bottle-websocket->eel)\n",
            "  Downloading zope.interface-6.4.post2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent->gevent-websocket->bottle-websocket->eel) (3.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.event->gevent->gevent-websocket->bottle-websocket->eel) (71.0.4)\n",
            "Downloading bottle-0.12.25-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.2/90.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\n",
            "Downloading gevent_websocket-0.10.1-py3-none-any.whl (22 kB)\n",
            "Downloading gevent-24.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading zope.interface-6.4.post2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: eel, bottle-websocket\n",
            "  Building wheel for eel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for eel: filename=Eel-0.17.0-py3-none-any.whl size=20605 sha256=42df38fea05de89bb4f81a7682847b5bc5ee1099de8876ed69a7c7da9e66ea46\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/29/f8/da1da953fa848caca0e988ae4de7f3eec46c0702615605bc4d\n",
            "  Building wheel for bottle-websocket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bottle-websocket: filename=bottle_websocket-0.2.9-py3-none-any.whl size=2332 sha256=82adba8bbf8bcdf2c22534134c184acf863eeaf7be7a765d2ac245eb95cb8813\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/9c/7d/5cc2fe1ff85ad654a0e86d72d1706a97ef15db2200b83c98d6\n",
            "Successfully built eel bottle-websocket\n",
            "Installing collected packages: whichcraft, bottle, zope.interface, zope.event, gevent, gevent-websocket, bottle-websocket, eel\n",
            "Successfully installed bottle-0.12.25 bottle-websocket-0.2.9 eel-0.17.0 gevent-24.2.1 gevent-websocket-0.10.1 whichcraft-0.6.1 zope.event-5.0 zope.interface-6.4.post2\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2024.7.4)\n",
            "Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.3\n",
            "    Uninstalling graphviz-0.20.3:\n",
            "      Successfully uninstalled graphviz-0.20.3\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n",
            "Collecting efficientnet\n",
            "  Downloading efficientnet-1.1.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from efficientnet) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.11.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (2024.7.24)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (0.4)\n",
            "Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-applications, efficientnet\n",
            "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.3.1 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tensorflow==2.15.0\n",
        "!pip3 install bcolz-zipline\n",
        "!pip install insightface\n",
        "!pip install lightgbm\n",
        "!pip install matplotlib\n",
        "!pip install mtcnn\n",
        "!pip install opencv-python\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm\n",
        "!pip install eel\n",
        "!pip install kaggle\n",
        "\n",
        "!pip install mxnet\n",
        "!pip install -U efficientnet\n",
        "!pip install protobuf\n",
        "!pip install tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsaiuQMb2Bld",
        "outputId": "f4f2e71a-0ec3-454a-a277-62ea316f286b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPUs are available:\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "TensorFlow version: 2.15.0\n",
            "Devices:\n",
            "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def check_gpu():\n",
        "    # Check if a GPU is available\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        print(\"GPUs are available:\")\n",
        "        for gpu in gpus:\n",
        "            print(gpu)\n",
        "    else:\n",
        "        print(\"No GPUs found.\")\n",
        "\n",
        "    # Print TensorFlow version\n",
        "    print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "    # Print device information\n",
        "    print(\"Devices:\")\n",
        "    for device in tf.config.experimental.list_physical_devices():\n",
        "        print(device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_gpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oprl1Jzm2_va",
        "outputId": "f5747122-fee5-4ec4-d7e1-865d12fa92ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ArcFace-tf2'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 50 (delta 14), reused 42 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (50/50), 2.98 MiB | 27.04 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tharindu326/ArcFace-tf2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7Ea35Zm3GUd",
        "outputId": "7faa2ac5-a015-49cc-b2f7-2f49e9cef106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ArcFace-tf2\n"
          ]
        }
      ],
      "source": [
        "%cd ArcFace-tf2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U6vrd4x4v5S",
        "outputId": "e373d6e5-4a41-4c39-baf8-ed1957ce8fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# get your kaggle.json first and put in \"ArcFace-tf2\" before running this\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HvqeC_Z3J7u",
        "outputId": "6d50747d-1c67-46ae-dbba-68941c1569cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/thnhnguyntrng/faces-ms1mrefinev2-112x112\n",
            "License(s): unknown\n",
            "Downloading faces-ms1mrefinev2-112x112.zip to /content/ArcFace-tf2\n",
            "100% 12.4G/12.4G [01:04<00:00, 254MB/s]\n",
            "100% 12.4G/12.4G [01:04<00:00, 206MB/s]\n"
          ]
        }
      ],
      "source": [
        "#!wget -O casia_webface.zip https://www.dropbox.com/s/wpx6tqjf0y5mf6r/faces_ms1m-refine-v2_112x112.zip?dl=1\n",
        "!kaggle datasets download -d thnhnguyntrng/faces-ms1mrefinev2-112x112"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR-5p8gv3L6i",
        "outputId": "c51f5191-1488-4798-f3fb-72ceeaadb8cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  faces-ms1mrefinev2-112x112.zip\n",
            "  inflating: faces_emore/agedb_30.bin  \n",
            "  inflating: faces_emore/calfw.bin   \n",
            "  inflating: faces_emore/cfp_ff.bin  \n",
            "  inflating: faces_emore/cfp_fp.bin  \n",
            "  inflating: faces_emore/cplfw.bin   \n",
            "  inflating: faces_emore/lfw.bin     \n",
            "  inflating: faces_emore/property    \n",
            "  inflating: faces_emore/train.idx   \n",
            "  inflating: faces_emore/train.rec   \n",
            "  inflating: faces_emore/vgg2_fp.bin  \n"
          ]
        }
      ],
      "source": [
        "# !unzip casia_webface.zip\n",
        "# !rm casia_webface.zip\n",
        "\n",
        "!unzip faces-ms1mrefinev2-112x112.zip\n",
        "!rm faces-ms1mrefinev2-112x112.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1jN9sFA3Nvd",
        "outputId": "7f28e0b5-d0d4-4946-f7b8-fa991e1ba9ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-04 14:30:24.625659: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-04 14:30:24.625715: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-04 14:30:24.626952: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-04 14:30:25.783035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "PRESS ENTER TO GO THROUGH THIS PROCESS\n",
            "[5822654. 5908396.]\n",
            "10000 num image processed\n",
            "20000 num image processed\n",
            "30000 num image processed\n",
            "40000 num image processed\n",
            "50000 num image processed\n",
            "60000 num image processed\n",
            "70000 num image processed\n",
            "80000 num image processed\n",
            "90000 num image processed\n",
            "100000 num image processed\n",
            "110000 num image processed\n",
            "120000 num image processed\n",
            "130000 num image processed\n",
            "140000 num image processed\n",
            "150000 num image processed\n",
            "160000 num image processed\n",
            "170000 num image processed\n",
            "180000 num image processed\n",
            "190000 num image processed\n",
            "200000 num image processed\n",
            "210000 num image processed\n",
            "220000 num image processed\n",
            "230000 num image processed\n",
            "240000 num image processed\n",
            "250000 num image processed\n",
            "260000 num image processed\n",
            "270000 num image processed\n",
            "280000 num image processed\n",
            "290000 num image processed\n",
            "300000 num image processed\n",
            "310000 num image processed\n",
            "320000 num image processed\n",
            "330000 num image processed\n",
            "340000 num image processed\n",
            "350000 num image processed\n",
            "360000 num image processed\n",
            "370000 num image processed\n",
            "380000 num image processed\n",
            "390000 num image processed\n",
            "400000 num image processed\n",
            "410000 num image processed\n",
            "420000 num image processed\n",
            "430000 num image processed\n",
            "440000 num image processed\n",
            "450000 num image processed\n",
            "460000 num image processed\n",
            "470000 num image processed\n",
            "480000 num image processed\n",
            "490000 num image processed\n",
            "500000 num image processed\n",
            "510000 num image processed\n",
            "520000 num image processed\n",
            "530000 num image processed\n",
            "540000 num image processed\n",
            "550000 num image processed\n",
            "560000 num image processed\n",
            "570000 num image processed\n",
            "580000 num image processed\n",
            "590000 num image processed\n",
            "600000 num image processed\n",
            "610000 num image processed\n",
            "620000 num image processed\n",
            "630000 num image processed\n",
            "640000 num image processed\n",
            "650000 num image processed\n",
            "660000 num image processed\n",
            "670000 num image processed\n",
            "680000 num image processed\n",
            "690000 num image processed\n",
            "700000 num image processed\n",
            "710000 num image processed\n",
            "720000 num image processed\n",
            "730000 num image processed\n",
            "740000 num image processed\n",
            "750000 num image processed\n",
            "760000 num image processed\n",
            "770000 num image processed\n",
            "780000 num image processed\n",
            "790000 num image processed\n",
            "800000 num image processed\n",
            "810000 num image processed\n",
            "820000 num image processed\n",
            "830000 num image processed\n",
            "840000 num image processed\n",
            "850000 num image processed\n",
            "860000 num image processed\n",
            "870000 num image processed\n",
            "880000 num image processed\n",
            "890000 num image processed\n",
            "900000 num image processed\n",
            "910000 num image processed\n",
            "920000 num image processed\n",
            "930000 num image processed\n",
            "940000 num image processed\n",
            "950000 num image processed\n",
            "960000 num image processed\n",
            "970000 num image processed\n",
            "980000 num image processed\n",
            "990000 num image processed\n",
            "1000000 num image processed\n",
            "1010000 num image processed\n",
            "1020000 num image processed\n",
            "1030000 num image processed\n",
            "1040000 num image processed\n",
            "1050000 num image processed\n",
            "1060000 num image processed\n",
            "1070000 num image processed\n",
            "1080000 num image processed\n",
            "1090000 num image processed\n",
            "1100000 num image processed\n",
            "1110000 num image processed\n",
            "1120000 num image processed\n",
            "1130000 num image processed\n",
            "1140000 num image processed\n",
            "1150000 num image processed\n",
            "1160000 num image processed\n",
            "1170000 num image processed\n",
            "1180000 num image processed\n",
            "1190000 num image processed\n",
            "1200000 num image processed\n",
            "1210000 num image processed\n",
            "1220000 num image processed\n",
            "1230000 num image processed\n",
            "1240000 num image processed\n",
            "1250000 num image processed\n",
            "1260000 num image processed\n",
            "1270000 num image processed\n",
            "1280000 num image processed\n",
            "1290000 num image processed\n",
            "1300000 num image processed\n",
            "1310000 num image processed\n",
            "1320000 num image processed\n",
            "1330000 num image processed\n",
            "1340000 num image processed\n",
            "1350000 num image processed\n",
            "1360000 num image processed\n",
            "1370000 num image processed\n",
            "1380000 num image processed\n",
            "1390000 num image processed\n",
            "1400000 num image processed\n",
            "1410000 num image processed\n",
            "1420000 num image processed\n",
            "1430000 num image processed\n",
            "1440000 num image processed\n",
            "1450000 num image processed\n",
            "1460000 num image processed\n",
            "1470000 num image processed\n",
            "1480000 num image processed\n",
            "1490000 num image processed\n",
            "1500000 num image processed\n",
            "1510000 num image processed\n",
            "1520000 num image processed\n",
            "1530000 num image processed\n",
            "1540000 num image processed\n",
            "1550000 num image processed\n",
            "1560000 num image processed\n",
            "1570000 num image processed\n",
            "1580000 num image processed\n",
            "1590000 num image processed\n",
            "1600000 num image processed\n",
            "1610000 num image processed\n",
            "1620000 num image processed\n",
            "1630000 num image processed\n",
            "1640000 num image processed\n",
            "1650000 num image processed\n",
            "1660000 num image processed\n",
            "1670000 num image processed\n",
            "1680000 num image processed\n",
            "1690000 num image processed\n",
            "1700000 num image processed\n",
            "1710000 num image processed\n",
            "1720000 num image processed\n",
            "1730000 num image processed\n",
            "1740000 num image processed\n",
            "1750000 num image processed\n",
            "1760000 num image processed\n",
            "1770000 num image processed\n",
            "1780000 num image processed\n",
            "1790000 num image processed\n",
            "1800000 num image processed\n",
            "1810000 num image processed\n",
            "1820000 num image processed\n",
            "1830000 num image processed\n",
            "1840000 num image processed\n",
            "1850000 num image processed\n",
            "1860000 num image processed\n",
            "1870000 num image processed\n",
            "1880000 num image processed\n",
            "1890000 num image processed\n",
            "1900000 num image processed\n",
            "1910000 num image processed\n",
            "1920000 num image processed\n",
            "1930000 num image processed\n",
            "1940000 num image processed\n",
            "1950000 num image processed\n",
            "1960000 num image processed\n",
            "1970000 num image processed\n",
            "1980000 num image processed\n",
            "1990000 num image processed\n",
            "2000000 num image processed\n",
            "2010000 num image processed\n",
            "2020000 num image processed\n",
            "2030000 num image processed\n",
            "2040000 num image processed\n",
            "2050000 num image processed\n",
            "2060000 num image processed\n",
            "2070000 num image processed\n",
            "2080000 num image processed\n",
            "2090000 num image processed\n",
            "2100000 num image processed\n",
            "2110000 num image processed\n",
            "2120000 num image processed\n",
            "2130000 num image processed\n",
            "2140000 num image processed\n",
            "2150000 num image processed\n",
            "2160000 num image processed\n",
            "2170000 num image processed\n",
            "2180000 num image processed\n",
            "2190000 num image processed\n",
            "2200000 num image processed\n",
            "2210000 num image processed\n",
            "2220000 num image processed\n",
            "2230000 num image processed\n",
            "2240000 num image processed\n",
            "2250000 num image processed\n",
            "2260000 num image processed\n",
            "2270000 num image processed\n",
            "2280000 num image processed\n",
            "2290000 num image processed\n",
            "2300000 num image processed\n",
            "2310000 num image processed\n",
            "2320000 num image processed\n",
            "2330000 num image processed\n",
            "2340000 num image processed\n",
            "2350000 num image processed\n",
            "2360000 num image processed\n",
            "2370000 num image processed\n",
            "2380000 num image processed\n",
            "2390000 num image processed\n",
            "2400000 num image processed\n",
            "2410000 num image processed\n",
            "2420000 num image processed\n",
            "2430000 num image processed\n",
            "2440000 num image processed\n",
            "2450000 num image processed\n",
            "2460000 num image processed\n",
            "2470000 num image processed\n",
            "2480000 num image processed\n",
            "2490000 num image processed\n",
            "2500000 num image processed\n",
            "2510000 num image processed\n",
            "2520000 num image processed\n",
            "2530000 num image processed\n",
            "2540000 num image processed\n",
            "2550000 num image processed\n",
            "2560000 num image processed\n",
            "2570000 num image processed\n",
            "2580000 num image processed\n",
            "2590000 num image processed\n",
            "2600000 num image processed\n",
            "2610000 num image processed\n",
            "2620000 num image processed\n",
            "2630000 num image processed\n",
            "2640000 num image processed\n",
            "2650000 num image processed\n",
            "2660000 num image processed\n",
            "2670000 num image processed\n",
            "2680000 num image processed\n",
            "2690000 num image processed\n",
            "2700000 num image processed\n",
            "2710000 num image processed\n",
            "2720000 num image processed\n",
            "2730000 num image processed\n",
            "2740000 num image processed\n",
            "2750000 num image processed\n",
            "2760000 num image processed\n",
            "2770000 num image processed\n",
            "2780000 num image processed\n",
            "2790000 num image processed\n",
            "2800000 num image processed\n",
            "2810000 num image processed\n",
            "2820000 num image processed\n",
            "2830000 num image processed\n",
            "2840000 num image processed\n",
            "2850000 num image processed\n",
            "2860000 num image processed\n",
            "2870000 num image processed\n",
            "2880000 num image processed\n",
            "2890000 num image processed\n",
            "2900000 num image processed\n",
            "2910000 num image processed\n",
            "2920000 num image processed\n",
            "2930000 num image processed\n",
            "2940000 num image processed\n",
            "2950000 num image processed\n",
            "2960000 num image processed\n",
            "2970000 num image processed\n",
            "2980000 num image processed\n",
            "2990000 num image processed\n",
            "3000000 num image processed\n",
            "3010000 num image processed\n",
            "3020000 num image processed\n",
            "3030000 num image processed\n",
            "3040000 num image processed\n",
            "3050000 num image processed\n",
            "3060000 num image processed\n",
            "3070000 num image processed\n",
            "3080000 num image processed\n",
            "3090000 num image processed\n",
            "3100000 num image processed\n",
            "3110000 num image processed\n",
            "3120000 num image processed\n",
            "3130000 num image processed\n",
            "3140000 num image processed\n",
            "3150000 num image processed\n",
            "3160000 num image processed\n",
            "3170000 num image processed\n",
            "3180000 num image processed\n",
            "3190000 num image processed\n",
            "3200000 num image processed\n",
            "3210000 num image processed\n",
            "3220000 num image processed\n",
            "3230000 num image processed\n",
            "3240000 num image processed\n",
            "3250000 num image processed\n",
            "3260000 num image processed\n",
            "3270000 num image processed\n",
            "3280000 num image processed\n",
            "3290000 num image processed\n",
            "3300000 num image processed\n",
            "3310000 num image processed\n",
            "3320000 num image processed\n",
            "3330000 num image processed\n",
            "3340000 num image processed\n",
            "3350000 num image processed\n",
            "3360000 num image processed\n",
            "3370000 num image processed\n",
            "3380000 num image processed\n",
            "3390000 num image processed\n",
            "3400000 num image processed\n",
            "3410000 num image processed\n",
            "3420000 num image processed\n",
            "3430000 num image processed\n",
            "3440000 num image processed\n",
            "3450000 num image processed\n",
            "3460000 num image processed\n",
            "3470000 num image processed\n",
            "3480000 num image processed\n",
            "3490000 num image processed\n",
            "3500000 num image processed\n",
            "3510000 num image processed\n",
            "3520000 num image processed\n",
            "3530000 num image processed\n",
            "3540000 num image processed\n",
            "3550000 num image processed\n",
            "3560000 num image processed\n",
            "3570000 num image processed\n",
            "3580000 num image processed\n",
            "3590000 num image processed\n",
            "3600000 num image processed\n",
            "3610000 num image processed\n",
            "3620000 num image processed\n",
            "3630000 num image processed\n",
            "3640000 num image processed\n",
            "3650000 num image processed\n",
            "3660000 num image processed\n",
            "3670000 num image processed\n",
            "3680000 num image processed\n",
            "3690000 num image processed\n",
            "3700000 num image processed\n",
            "3710000 num image processed\n",
            "3720000 num image processed\n",
            "3730000 num image processed\n",
            "3740000 num image processed\n",
            "3750000 num image processed\n",
            "3760000 num image processed\n",
            "3770000 num image processed\n",
            "3780000 num image processed\n",
            "3790000 num image processed\n",
            "3800000 num image processed\n",
            "3810000 num image processed\n",
            "3820000 num image processed\n",
            "3830000 num image processed\n",
            "3840000 num image processed\n",
            "3850000 num image processed\n",
            "3860000 num image processed\n",
            "3870000 num image processed\n",
            "3880000 num image processed\n",
            "3890000 num image processed\n",
            "3900000 num image processed\n",
            "3910000 num image processed\n",
            "3920000 num image processed\n",
            "3930000 num image processed\n",
            "3940000 num image processed\n",
            "3950000 num image processed\n",
            "3960000 num image processed\n",
            "3970000 num image processed\n",
            "3980000 num image processed\n",
            "3990000 num image processed\n",
            "4000000 num image processed\n",
            "4010000 num image processed\n",
            "4020000 num image processed\n",
            "4030000 num image processed\n",
            "4040000 num image processed\n",
            "4050000 num image processed\n",
            "4060000 num image processed\n",
            "4070000 num image processed\n",
            "4080000 num image processed\n",
            "4090000 num image processed\n",
            "4100000 num image processed\n",
            "4110000 num image processed\n",
            "4120000 num image processed\n",
            "4130000 num image processed\n",
            "4140000 num image processed\n",
            "4150000 num image processed\n",
            "4160000 num image processed\n",
            "4170000 num image processed\n",
            "4180000 num image processed\n",
            "4190000 num image processed\n",
            "4200000 num image processed\n",
            "4210000 num image processed\n",
            "4220000 num image processed\n",
            "4230000 num image processed\n",
            "4240000 num image processed\n",
            "4250000 num image processed\n",
            "4260000 num image processed\n",
            "4270000 num image processed\n",
            "4280000 num image processed\n",
            "4290000 num image processed\n",
            "4300000 num image processed\n",
            "4310000 num image processed\n",
            "4320000 num image processed\n",
            "4330000 num image processed\n",
            "4340000 num image processed\n",
            "4350000 num image processed\n",
            "4360000 num image processed\n",
            "4370000 num image processed\n",
            "4380000 num image processed\n",
            "4390000 num image processed\n",
            "4400000 num image processed\n",
            "4410000 num image processed\n",
            "4420000 num image processed\n",
            "4430000 num image processed\n",
            "4440000 num image processed\n",
            "4450000 num image processed\n",
            "4460000 num image processed\n",
            "4470000 num image processed\n",
            "4480000 num image processed\n",
            "4490000 num image processed\n",
            "4500000 num image processed\n",
            "4510000 num image processed\n",
            "4520000 num image processed\n",
            "4530000 num image processed\n",
            "4540000 num image processed\n",
            "4550000 num image processed\n",
            "4560000 num image processed\n",
            "4570000 num image processed\n",
            "4580000 num image processed\n",
            "4590000 num image processed\n",
            "4600000 num image processed\n",
            "4610000 num image processed\n",
            "4620000 num image processed\n",
            "4630000 num image processed\n",
            "4640000 num image processed\n",
            "4650000 num image processed\n",
            "4660000 num image processed\n",
            "4670000 num image processed\n",
            "4680000 num image processed\n",
            "4690000 num image processed\n",
            "4700000 num image processed\n",
            "4710000 num image processed\n",
            "4720000 num image processed\n",
            "4730000 num image processed\n",
            "4740000 num image processed\n",
            "4750000 num image processed\n",
            "4760000 num image processed\n",
            "4770000 num image processed\n",
            "4780000 num image processed\n",
            "4790000 num image processed\n",
            "4800000 num image processed\n",
            "4810000 num image processed\n",
            "4820000 num image processed\n",
            "4830000 num image processed\n",
            "4840000 num image processed\n",
            "4850000 num image processed\n",
            "4860000 num image processed\n",
            "4870000 num image processed\n",
            "4880000 num image processed\n",
            "4890000 num image processed\n",
            "4900000 num image processed\n",
            "4910000 num image processed\n",
            "4920000 num image processed\n",
            "4930000 num image processed\n",
            "4940000 num image processed\n",
            "4950000 num image processed\n",
            "4960000 num image processed\n",
            "4970000 num image processed\n",
            "4980000 num image processed\n",
            "4990000 num image processed\n",
            "5000000 num image processed\n",
            "5010000 num image processed\n",
            "5020000 num image processed\n",
            "5030000 num image processed\n",
            "5040000 num image processed\n",
            "5050000 num image processed\n",
            "5060000 num image processed\n",
            "5070000 num image processed\n",
            "5080000 num image processed\n",
            "5090000 num image processed\n",
            "5100000 num image processed\n",
            "5110000 num image processed\n",
            "5120000 num image processed\n",
            "5130000 num image processed\n",
            "5140000 num image processed\n",
            "5150000 num image processed\n",
            "5160000 num image processed\n",
            "5170000 num image processed\n",
            "5180000 num image processed\n",
            "5190000 num image processed\n",
            "5200000 num image processed\n",
            "5210000 num image processed\n",
            "5220000 num image processed\n",
            "5230000 num image processed\n",
            "5240000 num image processed\n",
            "5250000 num image processed\n",
            "5260000 num image processed\n",
            "5270000 num image processed\n",
            "5280000 num image processed\n",
            "5290000 num image processed\n",
            "5300000 num image processed\n",
            "5310000 num image processed\n",
            "5320000 num image processed\n",
            "5330000 num image processed\n",
            "5340000 num image processed\n",
            "5350000 num image processed\n",
            "5360000 num image processed\n",
            "5370000 num image processed\n",
            "5380000 num image processed\n",
            "5390000 num image processed\n",
            "5400000 num image processed\n",
            "5410000 num image processed\n",
            "5420000 num image processed\n",
            "5430000 num image processed\n",
            "5440000 num image processed\n",
            "5450000 num image processed\n",
            "5460000 num image processed\n",
            "5470000 num image processed\n",
            "5480000 num image processed\n",
            "5490000 num image processed\n",
            "5500000 num image processed\n",
            "5510000 num image processed\n",
            "5520000 num image processed\n",
            "5530000 num image processed\n",
            "5540000 num image processed\n",
            "5550000 num image processed\n",
            "5560000 num image processed\n",
            "5570000 num image processed\n",
            "5580000 num image processed\n",
            "5590000 num image processed\n",
            "5600000 num image processed\n",
            "5610000 num image processed\n",
            "5620000 num image processed\n",
            "5630000 num image processed\n",
            "5640000 num image processed\n",
            "5650000 num image processed\n",
            "5660000 num image processed\n",
            "5670000 num image processed\n",
            "5680000 num image processed\n",
            "5690000 num image processed\n",
            "5700000 num image processed\n",
            "5710000 num image processed\n",
            "5720000 num image processed\n",
            "5730000 num image processed\n",
            "5740000 num image processed\n",
            "5750000 num image processed\n",
            "5760000 num image processed\n",
            "5770000 num image processed\n",
            "5780000 num image processed\n",
            "5790000 num image processed\n",
            "5800000 num image processed\n",
            "5810000 num image processed\n",
            "5820000 num image processed\n"
          ]
        }
      ],
      "source": [
        "!python data_manager/turn_idx2tfrecord.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNRopFjRBUhq",
        "outputId": "46d8584e-7eb3-4eb7-c9f3-8730065d1c5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_obj = drive.CreateFile({'id': \"1WO5Meh_yAau00Gm2Rz2Pc0SRldLQYigT\"})\n",
        "file_obj.GetContentFile('lfw_align_112.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqJHRB6MBok2"
      },
      "outputs": [],
      "source": [
        "!unzip -q lfw_align_112.zip\n",
        "!rm lfw_align_112.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ38ErpCxlYu",
        "outputId": "eea86728-4a28-4251-909a-b9383eed460a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "id": "zMGo6b0pBqmy",
        "outputId": "9910c8d7-7ab4-4ee2-b927-64daa9c8e00d"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(8008, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir classifier_tensorboard --port 8008"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9cxxllIBt1j",
        "outputId": "0d3ecabf-5cac-47d8-c21d-8467cc008d1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-04 14:43:19.558601: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-04 14:43:19.558648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-04 14:43:19.559898: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-04 14:43:20.695339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-08-04 14:43:24.592599: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "[*] LEARNING RATE WILL BE CHECKED WHEN step\\alfa_divided_ten == 0\n",
            "[*] SGD chosen as optimizer\n",
            "[*] Kernel regularizer value set to --> 0.0005\n",
            "Model: \"InceptionResNetV1-ArcFace\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 112, 112, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " Conv2d_1a_3x3 (Conv2D)      (None, 55, 55, 32)           864       ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " Conv2d_1a_3x3_BatchNorm (B  (None, 55, 55, 32)           96        ['Conv2d_1a_3x3[0][0]']       \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Conv2d_1a_3x3_Activation (  (None, 55, 55, 32)           96800     ['Conv2d_1a_3x3_BatchNorm[0][0\n",
            " PReLU)                                                             ]']                           \n",
            "                                                                                                  \n",
            " Conv2d_2a_3x3 (Conv2D)      (None, 53, 53, 32)           9216      ['Conv2d_1a_3x3_Activation[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " Conv2d_2a_3x3_BatchNorm (B  (None, 53, 53, 32)           96        ['Conv2d_2a_3x3[0][0]']       \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Conv2d_2a_3x3_Activation (  (None, 53, 53, 32)           89888     ['Conv2d_2a_3x3_BatchNorm[0][0\n",
            " PReLU)                                                             ]']                           \n",
            "                                                                                                  \n",
            " Conv2d_2b_3x3 (Conv2D)      (None, 53, 53, 64)           18432     ['Conv2d_2a_3x3_Activation[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " Conv2d_2b_3x3_BatchNorm (B  (None, 53, 53, 64)           192       ['Conv2d_2b_3x3[0][0]']       \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Conv2d_2b_3x3_Activation (  (None, 53, 53, 64)           179776    ['Conv2d_2b_3x3_BatchNorm[0][0\n",
            " PReLU)                                                             ]']                           \n",
            "                                                                                                  \n",
            " MaxPool_3a_3x3 (MaxPooling  (None, 26, 26, 64)           0         ['Conv2d_2b_3x3_Activation[0][\n",
            " 2D)                                                                0]']                          \n",
            "                                                                                                  \n",
            " Conv2d_3b_1x1 (Conv2D)      (None, 26, 26, 80)           5120      ['MaxPool_3a_3x3[0][0]']      \n",
            "                                                                                                  \n",
            " Conv2d_3b_1x1_BatchNorm (B  (None, 26, 26, 80)           240       ['Conv2d_3b_1x1[0][0]']       \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Conv2d_3b_1x1_Activation (  (None, 26, 26, 80)           54080     ['Conv2d_3b_1x1_BatchNorm[0][0\n",
            " PReLU)                                                             ]']                           \n",
            "                                                                                                  \n",
            " Conv2d_4a_3x3 (Conv2D)      (None, 24, 24, 192)          138240    ['Conv2d_3b_1x1_Activation[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " Conv2d_4a_3x3_BatchNorm (B  (None, 24, 24, 192)          576       ['Conv2d_4a_3x3[0][0]']       \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Conv2d_4a_3x3_Activation (  (None, 24, 24, 192)          110592    ['Conv2d_4a_3x3_BatchNorm[0][0\n",
            " PReLU)                                                             ]']                           \n",
            "                                                                                                  \n",
            " Conv2d_4b_3x3 (Conv2D)      (None, 11, 11, 256)          442368    ['Conv2d_4a_3x3_Activation[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " Conv2d_4b_3x3_BatchNorm (B  (None, 11, 11, 256)          768       ['Conv2d_4b_3x3[0][0]']       \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " Conv2d_4b_3x3_Activation (  (None, 11, 11, 256)          30976     ['Conv2d_4b_3x3_BatchNorm[0][0\n",
            " PReLU)                                                             ]']                           \n",
            "                                                                                                  \n",
            " Block35_1_Branch_2_Conv2d_  (None, 11, 11, 32)           8192      ['Conv2d_4b_3x3_Activation[0][\n",
            " 0a_1x1 (Conv2D)                                                    0]']                          \n",
            "                                                                                                  \n",
            " Block35_1_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_1_Branch_2_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_1_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_1_Branch_2_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_1_Branch_1_Conv2d_  (None, 11, 11, 32)           8192      ['Conv2d_4b_3x3_Activation[0][\n",
            " 0a_1x1 (Conv2D)                                                    0]']                          \n",
            "                                                                                                  \n",
            " Block35_1_Branch_2_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_1_Branch_2_Conv2d_0a\n",
            " 0b_3x3 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_1_Branch_1_Conv2d_  (None, 11, 11, 32)           96        ['Block35_1_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_1_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_1_Branch_2_Conv2d_0b\n",
            " 0b_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_1_Branch_1_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_1_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_1_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_1_Branch_2_Conv2d_0b\n",
            " 0b_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_1_Branch_0_Conv2d_  (None, 11, 11, 32)           8192      ['Conv2d_4b_3x3_Activation[0][\n",
            " 1x1 (Conv2D)                                                       0]']                          \n",
            "                                                                                                  \n",
            " Block35_1_Branch_1_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_1_Branch_1_Conv2d_0a\n",
            " 0b_3x3 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_1_Branch_2_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_1_Branch_2_Conv2d_0b\n",
            " 0c_3x3 (Conv2D)                                                    _3x3_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_1_Branch_0_Conv2d_  (None, 11, 11, 32)           96        ['Block35_1_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block35_1_Branch_1_Conv2d_  (None, 11, 11, 32)           96        ['Block35_1_Branch_1_Conv2d_0b\n",
            " 0b_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_1_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_1_Branch_2_Conv2d_0c\n",
            " 0c_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_1_Branch_0_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_1_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block35_1_Branch_1_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_1_Branch_1_Conv2d_0b\n",
            " 0b_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_1_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_1_Branch_2_Conv2d_0c\n",
            " 0c_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_1_Concatenate (Con  (None, 11, 11, 96)           0         ['Block35_1_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block35_1_Branch_1_Conv2d_0b\n",
            "                                                                    _3x3_Activation[0][0]',       \n",
            "                                                                     'Block35_1_Branch_2_Conv2d_0c\n",
            "                                                                    _3x3_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_1_Conv2d_1x1 (Conv  (None, 11, 11, 256)          24832     ['Block35_1_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 11, 11, 256)          0         ['Block35_1_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add (Add)                   (None, 11, 11, 256)          0         ['Conv2d_4b_3x3_Activation[0][\n",
            "                                                                    0]',                          \n",
            "                                                                     'lambda[0][0]']              \n",
            "                                                                                                  \n",
            " Block35_1_Activation (PReL  (None, 11, 11, 256)          30976     ['add[0][0]']                 \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block35_2_Branch_2_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_1_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block35_2_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_2_Branch_2_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_2_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_2_Branch_2_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_2_Branch_1_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_1_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block35_2_Branch_2_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_2_Branch_2_Conv2d_0a\n",
            " 0b_3x3 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_2_Branch_1_Conv2d_  (None, 11, 11, 32)           96        ['Block35_2_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_2_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_2_Branch_2_Conv2d_0b\n",
            " 0b_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_2_Branch_1_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_2_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_2_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_2_Branch_2_Conv2d_0b\n",
            " 0b_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_2_Branch_0_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_1_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block35_2_Branch_1_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_2_Branch_1_Conv2d_0a\n",
            " 0b_3x3 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_2_Branch_2_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_2_Branch_2_Conv2d_0b\n",
            " 0c_3x3 (Conv2D)                                                    _3x3_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_2_Branch_0_Conv2d_  (None, 11, 11, 32)           96        ['Block35_2_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block35_2_Branch_1_Conv2d_  (None, 11, 11, 32)           96        ['Block35_2_Branch_1_Conv2d_0b\n",
            " 0b_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_2_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_2_Branch_2_Conv2d_0c\n",
            " 0c_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_2_Branch_0_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_2_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block35_2_Branch_1_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_2_Branch_1_Conv2d_0b\n",
            " 0b_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_2_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_2_Branch_2_Conv2d_0c\n",
            " 0c_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_2_Concatenate (Con  (None, 11, 11, 96)           0         ['Block35_2_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block35_2_Branch_1_Conv2d_0b\n",
            "                                                                    _3x3_Activation[0][0]',       \n",
            "                                                                     'Block35_2_Branch_2_Conv2d_0c\n",
            "                                                                    _3x3_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_2_Conv2d_1x1 (Conv  (None, 11, 11, 256)          24832     ['Block35_2_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)           (None, 11, 11, 256)          0         ['Block35_2_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 11, 11, 256)          0         ['Block35_1_Activation[0][0]',\n",
            "                                                                     'lambda_1[0][0]']            \n",
            "                                                                                                  \n",
            " Block35_2_Activation (PReL  (None, 11, 11, 256)          30976     ['add_1[0][0]']               \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block35_3_Branch_2_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_2_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block35_3_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_3_Branch_2_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_3_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_3_Branch_2_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_3_Branch_1_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_2_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block35_3_Branch_2_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_3_Branch_2_Conv2d_0a\n",
            " 0b_3x3 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_3_Branch_1_Conv2d_  (None, 11, 11, 32)           96        ['Block35_3_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_3_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_3_Branch_2_Conv2d_0b\n",
            " 0b_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_3_Branch_1_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_3_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_3_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_3_Branch_2_Conv2d_0b\n",
            " 0b_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_3_Branch_0_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_2_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block35_3_Branch_1_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_3_Branch_1_Conv2d_0a\n",
            " 0b_3x3 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_3_Branch_2_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_3_Branch_2_Conv2d_0b\n",
            " 0c_3x3 (Conv2D)                                                    _3x3_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_3_Branch_0_Conv2d_  (None, 11, 11, 32)           96        ['Block35_3_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block35_3_Branch_1_Conv2d_  (None, 11, 11, 32)           96        ['Block35_3_Branch_1_Conv2d_0b\n",
            " 0b_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_3_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_3_Branch_2_Conv2d_0c\n",
            " 0c_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_3_Branch_0_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_3_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block35_3_Branch_1_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_3_Branch_1_Conv2d_0b\n",
            " 0b_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_3_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_3_Branch_2_Conv2d_0c\n",
            " 0c_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_3_Concatenate (Con  (None, 11, 11, 96)           0         ['Block35_3_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block35_3_Branch_1_Conv2d_0b\n",
            "                                                                    _3x3_Activation[0][0]',       \n",
            "                                                                     'Block35_3_Branch_2_Conv2d_0c\n",
            "                                                                    _3x3_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_3_Conv2d_1x1 (Conv  (None, 11, 11, 256)          24832     ['Block35_3_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)           (None, 11, 11, 256)          0         ['Block35_3_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 11, 11, 256)          0         ['Block35_2_Activation[0][0]',\n",
            "                                                                     'lambda_2[0][0]']            \n",
            "                                                                                                  \n",
            " Block35_3_Activation (PReL  (None, 11, 11, 256)          30976     ['add_2[0][0]']               \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block35_4_Branch_2_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_3_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block35_4_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_4_Branch_2_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_4_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_4_Branch_2_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_4_Branch_1_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_3_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block35_4_Branch_2_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_4_Branch_2_Conv2d_0a\n",
            " 0b_3x3 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_4_Branch_1_Conv2d_  (None, 11, 11, 32)           96        ['Block35_4_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_4_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_4_Branch_2_Conv2d_0b\n",
            " 0b_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_4_Branch_1_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_4_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_4_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_4_Branch_2_Conv2d_0b\n",
            " 0b_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_4_Branch_0_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_3_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block35_4_Branch_1_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_4_Branch_1_Conv2d_0a\n",
            " 0b_3x3 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_4_Branch_2_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_4_Branch_2_Conv2d_0b\n",
            " 0c_3x3 (Conv2D)                                                    _3x3_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_4_Branch_0_Conv2d_  (None, 11, 11, 32)           96        ['Block35_4_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block35_4_Branch_1_Conv2d_  (None, 11, 11, 32)           96        ['Block35_4_Branch_1_Conv2d_0b\n",
            " 0b_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_4_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_4_Branch_2_Conv2d_0c\n",
            " 0c_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_4_Branch_0_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_4_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block35_4_Branch_1_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_4_Branch_1_Conv2d_0b\n",
            " 0b_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_4_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_4_Branch_2_Conv2d_0c\n",
            " 0c_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_4_Concatenate (Con  (None, 11, 11, 96)           0         ['Block35_4_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block35_4_Branch_1_Conv2d_0b\n",
            "                                                                    _3x3_Activation[0][0]',       \n",
            "                                                                     'Block35_4_Branch_2_Conv2d_0c\n",
            "                                                                    _3x3_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_4_Conv2d_1x1 (Conv  (None, 11, 11, 256)          24832     ['Block35_4_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)           (None, 11, 11, 256)          0         ['Block35_4_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 11, 11, 256)          0         ['Block35_3_Activation[0][0]',\n",
            "                                                                     'lambda_3[0][0]']            \n",
            "                                                                                                  \n",
            " Block35_4_Activation (PReL  (None, 11, 11, 256)          30976     ['add_3[0][0]']               \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block35_5_Branch_2_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_4_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block35_5_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_5_Branch_2_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_5_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_5_Branch_2_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_5_Branch_1_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_4_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block35_5_Branch_2_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_5_Branch_2_Conv2d_0a\n",
            " 0b_3x3 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_5_Branch_1_Conv2d_  (None, 11, 11, 32)           96        ['Block35_5_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_5_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_5_Branch_2_Conv2d_0b\n",
            " 0b_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_5_Branch_1_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_5_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_5_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_5_Branch_2_Conv2d_0b\n",
            " 0b_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_5_Branch_0_Conv2d_  (None, 11, 11, 32)           8192      ['Block35_4_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block35_5_Branch_1_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_5_Branch_1_Conv2d_0a\n",
            " 0b_3x3 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_5_Branch_2_Conv2d_  (None, 11, 11, 32)           9216      ['Block35_5_Branch_2_Conv2d_0b\n",
            " 0c_3x3 (Conv2D)                                                    _3x3_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_5_Branch_0_Conv2d_  (None, 11, 11, 32)           96        ['Block35_5_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block35_5_Branch_1_Conv2d_  (None, 11, 11, 32)           96        ['Block35_5_Branch_1_Conv2d_0b\n",
            " 0b_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_5_Branch_2_Conv2d_  (None, 11, 11, 32)           96        ['Block35_5_Branch_2_Conv2d_0c\n",
            " 0c_3x3_BatchNorm (BatchNor                                         _3x3[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block35_5_Branch_0_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_5_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block35_5_Branch_1_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_5_Branch_1_Conv2d_0b\n",
            " 0b_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_5_Branch_2_Conv2d_  (None, 11, 11, 32)           3872      ['Block35_5_Branch_2_Conv2d_0c\n",
            " 0c_3x3_Activation (PReLU)                                          _3x3_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block35_5_Concatenate (Con  (None, 11, 11, 96)           0         ['Block35_5_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block35_5_Branch_1_Conv2d_0b\n",
            "                                                                    _3x3_Activation[0][0]',       \n",
            "                                                                     'Block35_5_Branch_2_Conv2d_0c\n",
            "                                                                    _3x3_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block35_5_Conv2d_1x1 (Conv  (None, 11, 11, 256)          24832     ['Block35_5_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)           (None, 11, 11, 256)          0         ['Block35_5_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 11, 11, 256)          0         ['Block35_4_Activation[0][0]',\n",
            "                                                                     'lambda_4[0][0]']            \n",
            "                                                                                                  \n",
            " Block35_5_Activation (PReL  (None, 11, 11, 256)          30976     ['add_4[0][0]']               \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_1_Conv2d_0  (None, 11, 11, 192)          49152     ['Block35_5_Activation[0][0]']\n",
            " a_1x1 (Conv2D)                                                                                   \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_1_Conv2d_0  (None, 11, 11, 192)          576       ['Mixed_6a_Branch_1_Conv2d_0a_\n",
            " a_1x1_BatchNorm (BatchNorm                                         1x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_1_Conv2d_0  (None, 11, 11, 192)          23232     ['Mixed_6a_Branch_1_Conv2d_0a_\n",
            " a_1x1_Activation (PReLU)                                           1x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_1_Conv2d_0  (None, 11, 11, 192)          331776    ['Mixed_6a_Branch_1_Conv2d_0a_\n",
            " b_3x3 (Conv2D)                                                     1x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_1_Conv2d_0  (None, 11, 11, 192)          576       ['Mixed_6a_Branch_1_Conv2d_0b_\n",
            " b_3x3_BatchNorm (BatchNorm                                         3x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_1_Conv2d_0  (None, 11, 11, 192)          23232     ['Mixed_6a_Branch_1_Conv2d_0b_\n",
            " b_3x3_Activation (PReLU)                                           3x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_0_Conv2d_1  (None, 5, 5, 384)            884736    ['Block35_5_Activation[0][0]']\n",
            " a_3x3 (Conv2D)                                                                                   \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_1_Conv2d_1  (None, 5, 5, 256)            442368    ['Mixed_6a_Branch_1_Conv2d_0b_\n",
            " a_3x3 (Conv2D)                                                     3x3_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_0_Conv2d_1  (None, 5, 5, 384)            1152      ['Mixed_6a_Branch_0_Conv2d_1a_\n",
            " a_3x3_BatchNorm (BatchNorm                                         3x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_1_Conv2d_1  (None, 5, 5, 256)            768       ['Mixed_6a_Branch_1_Conv2d_1a_\n",
            " a_3x3_BatchNorm (BatchNorm                                         3x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_0_Conv2d_1  (None, 5, 5, 384)            9600      ['Mixed_6a_Branch_0_Conv2d_1a_\n",
            " a_3x3_Activation (PReLU)                                           3x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_1_Conv2d_1  (None, 5, 5, 256)            6400      ['Mixed_6a_Branch_1_Conv2d_1a_\n",
            " a_3x3_Activation (PReLU)                                           3x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_6a_Branch_2_MaxPool_  (None, 5, 5, 256)            0         ['Block35_5_Activation[0][0]']\n",
            " 1a_3x3 (MaxPooling2D)                                                                            \n",
            "                                                                                                  \n",
            " Mixed_6a (Concatenate)      (None, 5, 5, 896)            0         ['Mixed_6a_Branch_0_Conv2d_1a_\n",
            "                                                                    3x3_Activation[0][0]',        \n",
            "                                                                     'Mixed_6a_Branch_1_Conv2d_1a_\n",
            "                                                                    3x3_Activation[0][0]',        \n",
            "                                                                     'Mixed_6a_Branch_2_MaxPool_1a\n",
            "                                                                    _3x3[0][0]']                  \n",
            "                                                                                                  \n",
            " Block17_1_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Mixed_6a[0][0]']            \n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block17_1_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_1_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_1_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_1_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_1_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_1_Branch_1_Conv2d_0a\n",
            " 0b_1x7 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_1_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_1_Branch_1_Conv2d_0b\n",
            " 0b_1x7_BatchNorm (BatchNor                                         _1x7[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_1_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_1_Branch_1_Conv2d_0b\n",
            " 0b_1x7_Activation (PReLU)                                          _1x7_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_1_Branch_0_Conv2d_  (None, 5, 5, 128)            114688    ['Mixed_6a[0][0]']            \n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_1_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_1_Branch_1_Conv2d_0b\n",
            " 0c_7x1 (Conv2D)                                                    _1x7_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_1_Branch_0_Conv2d_  (None, 5, 5, 128)            384       ['Block17_1_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block17_1_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_1_Branch_1_Conv2d_0c\n",
            " 0c_7x1_BatchNorm (BatchNor                                         _7x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_1_Branch_0_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_1_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_1_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_1_Branch_1_Conv2d_0c\n",
            " 0c_7x1_Activation (PReLU)                                          _7x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_1_Concatenate (Con  (None, 5, 5, 256)            0         ['Block17_1_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block17_1_Branch_1_Conv2d_0c\n",
            "                                                                    _7x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_1_Conv2d_1x1 (Conv  (None, 5, 5, 896)            230272    ['Block17_1_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)           (None, 5, 5, 896)            0         ['Block17_1_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 5, 5, 896)            0         ['Mixed_6a[0][0]',            \n",
            "                                                                     'lambda_5[0][0]']            \n",
            "                                                                                                  \n",
            " Block17_1_Activation (PReL  (None, 5, 5, 896)            22400     ['add_5[0][0]']               \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block17_2_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_1_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block17_2_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_2_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_2_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_2_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_2_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_2_Branch_1_Conv2d_0a\n",
            " 0b_1x7 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_2_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_2_Branch_1_Conv2d_0b\n",
            " 0b_1x7_BatchNorm (BatchNor                                         _1x7[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_2_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_2_Branch_1_Conv2d_0b\n",
            " 0b_1x7_Activation (PReLU)                                          _1x7_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_2_Branch_0_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_1_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_2_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_2_Branch_1_Conv2d_0b\n",
            " 0c_7x1 (Conv2D)                                                    _1x7_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_2_Branch_0_Conv2d_  (None, 5, 5, 128)            384       ['Block17_2_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block17_2_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_2_Branch_1_Conv2d_0c\n",
            " 0c_7x1_BatchNorm (BatchNor                                         _7x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_2_Branch_0_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_2_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_2_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_2_Branch_1_Conv2d_0c\n",
            " 0c_7x1_Activation (PReLU)                                          _7x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_2_Concatenate (Con  (None, 5, 5, 256)            0         ['Block17_2_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block17_2_Branch_1_Conv2d_0c\n",
            "                                                                    _7x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_2_Conv2d_1x1 (Conv  (None, 5, 5, 896)            230272    ['Block17_2_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)           (None, 5, 5, 896)            0         ['Block17_2_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 5, 5, 896)            0         ['Block17_1_Activation[0][0]',\n",
            "                                                                     'lambda_6[0][0]']            \n",
            "                                                                                                  \n",
            " Block17_2_Activation (PReL  (None, 5, 5, 896)            22400     ['add_6[0][0]']               \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block17_3_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_2_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block17_3_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_3_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_3_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_3_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_3_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_3_Branch_1_Conv2d_0a\n",
            " 0b_1x7 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_3_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_3_Branch_1_Conv2d_0b\n",
            " 0b_1x7_BatchNorm (BatchNor                                         _1x7[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_3_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_3_Branch_1_Conv2d_0b\n",
            " 0b_1x7_Activation (PReLU)                                          _1x7_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_3_Branch_0_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_2_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_3_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_3_Branch_1_Conv2d_0b\n",
            " 0c_7x1 (Conv2D)                                                    _1x7_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_3_Branch_0_Conv2d_  (None, 5, 5, 128)            384       ['Block17_3_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block17_3_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_3_Branch_1_Conv2d_0c\n",
            " 0c_7x1_BatchNorm (BatchNor                                         _7x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_3_Branch_0_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_3_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_3_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_3_Branch_1_Conv2d_0c\n",
            " 0c_7x1_Activation (PReLU)                                          _7x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_3_Concatenate (Con  (None, 5, 5, 256)            0         ['Block17_3_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block17_3_Branch_1_Conv2d_0c\n",
            "                                                                    _7x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_3_Conv2d_1x1 (Conv  (None, 5, 5, 896)            230272    ['Block17_3_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)           (None, 5, 5, 896)            0         ['Block17_3_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 5, 5, 896)            0         ['Block17_2_Activation[0][0]',\n",
            "                                                                     'lambda_7[0][0]']            \n",
            "                                                                                                  \n",
            " Block17_3_Activation (PReL  (None, 5, 5, 896)            22400     ['add_7[0][0]']               \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block17_4_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_3_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block17_4_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_4_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_4_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_4_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_4_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_4_Branch_1_Conv2d_0a\n",
            " 0b_1x7 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_4_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_4_Branch_1_Conv2d_0b\n",
            " 0b_1x7_BatchNorm (BatchNor                                         _1x7[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_4_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_4_Branch_1_Conv2d_0b\n",
            " 0b_1x7_Activation (PReLU)                                          _1x7_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_4_Branch_0_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_3_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_4_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_4_Branch_1_Conv2d_0b\n",
            " 0c_7x1 (Conv2D)                                                    _1x7_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_4_Branch_0_Conv2d_  (None, 5, 5, 128)            384       ['Block17_4_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block17_4_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_4_Branch_1_Conv2d_0c\n",
            " 0c_7x1_BatchNorm (BatchNor                                         _7x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_4_Branch_0_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_4_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_4_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_4_Branch_1_Conv2d_0c\n",
            " 0c_7x1_Activation (PReLU)                                          _7x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_4_Concatenate (Con  (None, 5, 5, 256)            0         ['Block17_4_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block17_4_Branch_1_Conv2d_0c\n",
            "                                                                    _7x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_4_Conv2d_1x1 (Conv  (None, 5, 5, 896)            230272    ['Block17_4_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)           (None, 5, 5, 896)            0         ['Block17_4_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 5, 5, 896)            0         ['Block17_3_Activation[0][0]',\n",
            "                                                                     'lambda_8[0][0]']            \n",
            "                                                                                                  \n",
            " Block17_4_Activation (PReL  (None, 5, 5, 896)            22400     ['add_8[0][0]']               \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block17_5_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_4_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block17_5_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_5_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_5_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_5_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_5_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_5_Branch_1_Conv2d_0a\n",
            " 0b_1x7 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_5_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_5_Branch_1_Conv2d_0b\n",
            " 0b_1x7_BatchNorm (BatchNor                                         _1x7[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_5_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_5_Branch_1_Conv2d_0b\n",
            " 0b_1x7_Activation (PReLU)                                          _1x7_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_5_Branch_0_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_4_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_5_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_5_Branch_1_Conv2d_0b\n",
            " 0c_7x1 (Conv2D)                                                    _1x7_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_5_Branch_0_Conv2d_  (None, 5, 5, 128)            384       ['Block17_5_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block17_5_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_5_Branch_1_Conv2d_0c\n",
            " 0c_7x1_BatchNorm (BatchNor                                         _7x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_5_Branch_0_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_5_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_5_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_5_Branch_1_Conv2d_0c\n",
            " 0c_7x1_Activation (PReLU)                                          _7x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_5_Concatenate (Con  (None, 5, 5, 256)            0         ['Block17_5_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block17_5_Branch_1_Conv2d_0c\n",
            "                                                                    _7x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_5_Conv2d_1x1 (Conv  (None, 5, 5, 896)            230272    ['Block17_5_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)           (None, 5, 5, 896)            0         ['Block17_5_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 5, 5, 896)            0         ['Block17_4_Activation[0][0]',\n",
            "                                                                     'lambda_9[0][0]']            \n",
            "                                                                                                  \n",
            " Block17_5_Activation (PReL  (None, 5, 5, 896)            22400     ['add_9[0][0]']               \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block17_6_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_5_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block17_6_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_6_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_6_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_6_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_6_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_6_Branch_1_Conv2d_0a\n",
            " 0b_1x7 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_6_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_6_Branch_1_Conv2d_0b\n",
            " 0b_1x7_BatchNorm (BatchNor                                         _1x7[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_6_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_6_Branch_1_Conv2d_0b\n",
            " 0b_1x7_Activation (PReLU)                                          _1x7_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_6_Branch_0_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_5_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_6_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_6_Branch_1_Conv2d_0b\n",
            " 0c_7x1 (Conv2D)                                                    _1x7_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_6_Branch_0_Conv2d_  (None, 5, 5, 128)            384       ['Block17_6_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block17_6_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_6_Branch_1_Conv2d_0c\n",
            " 0c_7x1_BatchNorm (BatchNor                                         _7x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_6_Branch_0_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_6_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_6_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_6_Branch_1_Conv2d_0c\n",
            " 0c_7x1_Activation (PReLU)                                          _7x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_6_Concatenate (Con  (None, 5, 5, 256)            0         ['Block17_6_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block17_6_Branch_1_Conv2d_0c\n",
            "                                                                    _7x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_6_Conv2d_1x1 (Conv  (None, 5, 5, 896)            230272    ['Block17_6_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)          (None, 5, 5, 896)            0         ['Block17_6_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 5, 5, 896)            0         ['Block17_5_Activation[0][0]',\n",
            "                                                                     'lambda_10[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_6_Activation (PReL  (None, 5, 5, 896)            22400     ['add_10[0][0]']              \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block17_7_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_6_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block17_7_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_7_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_7_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_7_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_7_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_7_Branch_1_Conv2d_0a\n",
            " 0b_1x7 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_7_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_7_Branch_1_Conv2d_0b\n",
            " 0b_1x7_BatchNorm (BatchNor                                         _1x7[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_7_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_7_Branch_1_Conv2d_0b\n",
            " 0b_1x7_Activation (PReLU)                                          _1x7_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_7_Branch_0_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_6_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_7_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_7_Branch_1_Conv2d_0b\n",
            " 0c_7x1 (Conv2D)                                                    _1x7_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_7_Branch_0_Conv2d_  (None, 5, 5, 128)            384       ['Block17_7_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block17_7_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_7_Branch_1_Conv2d_0c\n",
            " 0c_7x1_BatchNorm (BatchNor                                         _7x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_7_Branch_0_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_7_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_7_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_7_Branch_1_Conv2d_0c\n",
            " 0c_7x1_Activation (PReLU)                                          _7x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_7_Concatenate (Con  (None, 5, 5, 256)            0         ['Block17_7_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block17_7_Branch_1_Conv2d_0c\n",
            "                                                                    _7x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_7_Conv2d_1x1 (Conv  (None, 5, 5, 896)            230272    ['Block17_7_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)          (None, 5, 5, 896)            0         ['Block17_7_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 5, 5, 896)            0         ['Block17_6_Activation[0][0]',\n",
            "                                                                     'lambda_11[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_7_Activation (PReL  (None, 5, 5, 896)            22400     ['add_11[0][0]']              \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block17_8_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_7_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block17_8_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_8_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_8_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_8_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_8_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_8_Branch_1_Conv2d_0a\n",
            " 0b_1x7 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_8_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_8_Branch_1_Conv2d_0b\n",
            " 0b_1x7_BatchNorm (BatchNor                                         _1x7[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_8_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_8_Branch_1_Conv2d_0b\n",
            " 0b_1x7_Activation (PReLU)                                          _1x7_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_8_Branch_0_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_7_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_8_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_8_Branch_1_Conv2d_0b\n",
            " 0c_7x1 (Conv2D)                                                    _1x7_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_8_Branch_0_Conv2d_  (None, 5, 5, 128)            384       ['Block17_8_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block17_8_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_8_Branch_1_Conv2d_0c\n",
            " 0c_7x1_BatchNorm (BatchNor                                         _7x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_8_Branch_0_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_8_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_8_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_8_Branch_1_Conv2d_0c\n",
            " 0c_7x1_Activation (PReLU)                                          _7x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_8_Concatenate (Con  (None, 5, 5, 256)            0         ['Block17_8_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block17_8_Branch_1_Conv2d_0c\n",
            "                                                                    _7x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_8_Conv2d_1x1 (Conv  (None, 5, 5, 896)            230272    ['Block17_8_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)          (None, 5, 5, 896)            0         ['Block17_8_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 5, 5, 896)            0         ['Block17_7_Activation[0][0]',\n",
            "                                                                     'lambda_12[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_8_Activation (PReL  (None, 5, 5, 896)            22400     ['add_12[0][0]']              \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block17_9_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_8_Activation[0][0]']\n",
            " 0a_1x1 (Conv2D)                                                                                  \n",
            "                                                                                                  \n",
            " Block17_9_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_9_Branch_1_Conv2d_0a\n",
            " 0a_1x1_BatchNorm (BatchNor                                         _1x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_9_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_9_Branch_1_Conv2d_0a\n",
            " 0a_1x1_Activation (PReLU)                                          _1x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_9_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_9_Branch_1_Conv2d_0a\n",
            " 0b_1x7 (Conv2D)                                                    _1x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_9_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_9_Branch_1_Conv2d_0b\n",
            " 0b_1x7_BatchNorm (BatchNor                                         _1x7[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_9_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_9_Branch_1_Conv2d_0b\n",
            " 0b_1x7_Activation (PReLU)                                          _1x7_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_9_Branch_0_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_8_Activation[0][0]']\n",
            " 1x1 (Conv2D)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_9_Branch_1_Conv2d_  (None, 5, 5, 128)            114688    ['Block17_9_Branch_1_Conv2d_0b\n",
            " 0c_7x1 (Conv2D)                                                    _1x7_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_9_Branch_0_Conv2d_  (None, 5, 5, 128)            384       ['Block17_9_Branch_0_Conv2d_1x\n",
            " 1x1_BatchNorm (BatchNormal                                         1[0][0]']                     \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " Block17_9_Branch_1_Conv2d_  (None, 5, 5, 128)            384       ['Block17_9_Branch_1_Conv2d_0c\n",
            " 0c_7x1_BatchNorm (BatchNor                                         _7x1[0][0]']                  \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " Block17_9_Branch_0_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_9_Branch_0_Conv2d_1x\n",
            " 1x1_Activation (PReLU)                                             1_BatchNorm[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_9_Branch_1_Conv2d_  (None, 5, 5, 128)            3200      ['Block17_9_Branch_1_Conv2d_0c\n",
            " 0c_7x1_Activation (PReLU)                                          _7x1_BatchNorm[0][0]']        \n",
            "                                                                                                  \n",
            " Block17_9_Concatenate (Con  (None, 5, 5, 256)            0         ['Block17_9_Branch_0_Conv2d_1x\n",
            " catenate)                                                          1_Activation[0][0]',          \n",
            "                                                                     'Block17_9_Branch_1_Conv2d_0c\n",
            "                                                                    _7x1_Activation[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_9_Conv2d_1x1 (Conv  (None, 5, 5, 896)            230272    ['Block17_9_Concatenate[0][0]'\n",
            " 2D)                                                                ]                             \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)          (None, 5, 5, 896)            0         ['Block17_9_Conv2d_1x1[0][0]']\n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 5, 5, 896)            0         ['Block17_8_Activation[0][0]',\n",
            "                                                                     'lambda_13[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_9_Activation (PReL  (None, 5, 5, 896)            22400     ['add_13[0][0]']              \n",
            " U)                                                                                               \n",
            "                                                                                                  \n",
            " Block17_10_Branch_1_Conv2d  (None, 5, 5, 128)            114688    ['Block17_9_Activation[0][0]']\n",
            " _0a_1x1 (Conv2D)                                                                                 \n",
            "                                                                                                  \n",
            " Block17_10_Branch_1_Conv2d  (None, 5, 5, 128)            384       ['Block17_10_Branch_1_Conv2d_0\n",
            " _0a_1x1_BatchNorm (BatchNo                                         a_1x1[0][0]']                 \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_10_Branch_1_Conv2d  (None, 5, 5, 128)            3200      ['Block17_10_Branch_1_Conv2d_0\n",
            " _0a_1x1_Activation (PReLU)                                         a_1x1_BatchNorm[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_10_Branch_1_Conv2d  (None, 5, 5, 128)            114688    ['Block17_10_Branch_1_Conv2d_0\n",
            " _0b_1x7 (Conv2D)                                                   a_1x1_Activation[0][0]']      \n",
            "                                                                                                  \n",
            " Block17_10_Branch_1_Conv2d  (None, 5, 5, 128)            384       ['Block17_10_Branch_1_Conv2d_0\n",
            " _0b_1x7_BatchNorm (BatchNo                                         b_1x7[0][0]']                 \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_10_Branch_1_Conv2d  (None, 5, 5, 128)            3200      ['Block17_10_Branch_1_Conv2d_0\n",
            " _0b_1x7_Activation (PReLU)                                         b_1x7_BatchNorm[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_10_Branch_0_Conv2d  (None, 5, 5, 128)            114688    ['Block17_9_Activation[0][0]']\n",
            " _1x1 (Conv2D)                                                                                    \n",
            "                                                                                                  \n",
            " Block17_10_Branch_1_Conv2d  (None, 5, 5, 128)            114688    ['Block17_10_Branch_1_Conv2d_0\n",
            " _0c_7x1 (Conv2D)                                                   b_1x7_Activation[0][0]']      \n",
            "                                                                                                  \n",
            " Block17_10_Branch_0_Conv2d  (None, 5, 5, 128)            384       ['Block17_10_Branch_0_Conv2d_1\n",
            " _1x1_BatchNorm (BatchNorma                                         x1[0][0]']                    \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " Block17_10_Branch_1_Conv2d  (None, 5, 5, 128)            384       ['Block17_10_Branch_1_Conv2d_0\n",
            " _0c_7x1_BatchNorm (BatchNo                                         c_7x1[0][0]']                 \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " Block17_10_Branch_0_Conv2d  (None, 5, 5, 128)            3200      ['Block17_10_Branch_0_Conv2d_1\n",
            " _1x1_Activation (PReLU)                                            x1_BatchNorm[0][0]']          \n",
            "                                                                                                  \n",
            " Block17_10_Branch_1_Conv2d  (None, 5, 5, 128)            3200      ['Block17_10_Branch_1_Conv2d_0\n",
            " _0c_7x1_Activation (PReLU)                                         c_7x1_BatchNorm[0][0]']       \n",
            "                                                                                                  \n",
            " Block17_10_Concatenate (Co  (None, 5, 5, 256)            0         ['Block17_10_Branch_0_Conv2d_1\n",
            " ncatenate)                                                         x1_Activation[0][0]',         \n",
            "                                                                     'Block17_10_Branch_1_Conv2d_0\n",
            "                                                                    c_7x1_Activation[0][0]']      \n",
            "                                                                                                  \n",
            " Block17_10_Conv2d_1x1 (Con  (None, 5, 5, 896)            230272    ['Block17_10_Concatenate[0][0]\n",
            " v2D)                                                               ']                            \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)          (None, 5, 5, 896)            0         ['Block17_10_Conv2d_1x1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_14 (Add)                (None, 5, 5, 896)            0         ['Block17_9_Activation[0][0]',\n",
            "                                                                     'lambda_14[0][0]']           \n",
            "                                                                                                  \n",
            " Block17_10_Activation (PRe  (None, 5, 5, 896)            22400     ['add_14[0][0]']              \n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_2_Conv2d_0  (None, 5, 5, 256)            229376    ['Block17_10_Activation[0][0]'\n",
            " a_1x1 (Conv2D)                                                     ]                             \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_2_Conv2d_0  (None, 5, 5, 256)            768       ['Mixed_7a_Branch_2_Conv2d_0a_\n",
            " a_1x1_BatchNorm (BatchNorm                                         1x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_2_Conv2d_0  (None, 5, 5, 256)            6400      ['Mixed_7a_Branch_2_Conv2d_0a_\n",
            " a_1x1_Activation (PReLU)                                           1x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_0_Conv2d_0  (None, 5, 5, 256)            229376    ['Block17_10_Activation[0][0]'\n",
            " a_1x1 (Conv2D)                                                     ]                             \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_1_Conv2d_0  (None, 5, 5, 256)            229376    ['Block17_10_Activation[0][0]'\n",
            " a_1x1 (Conv2D)                                                     ]                             \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_2_Conv2d_0  (None, 5, 5, 256)            589824    ['Mixed_7a_Branch_2_Conv2d_0a_\n",
            " b_3x3 (Conv2D)                                                     1x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_0_Conv2d_0  (None, 5, 5, 256)            768       ['Mixed_7a_Branch_0_Conv2d_0a_\n",
            " a_1x1_BatchNorm (BatchNorm                                         1x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_1_Conv2d_0  (None, 5, 5, 256)            768       ['Mixed_7a_Branch_1_Conv2d_0a_\n",
            " a_1x1_BatchNorm (BatchNorm                                         1x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_2_Conv2d_0  (None, 5, 5, 256)            768       ['Mixed_7a_Branch_2_Conv2d_0b_\n",
            " b_3x3_BatchNorm (BatchNorm                                         3x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_0_Conv2d_0  (None, 5, 5, 256)            6400      ['Mixed_7a_Branch_0_Conv2d_0a_\n",
            " a_1x1_Activation (PReLU)                                           1x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_1_Conv2d_0  (None, 5, 5, 256)            6400      ['Mixed_7a_Branch_1_Conv2d_0a_\n",
            " a_1x1_Activation (PReLU)                                           1x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_2_Conv2d_0  (None, 5, 5, 256)            6400      ['Mixed_7a_Branch_2_Conv2d_0b_\n",
            " b_3x3_Activation (PReLU)                                           3x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_0_Conv2d_1  (None, 2, 2, 384)            884736    ['Mixed_7a_Branch_0_Conv2d_0a_\n",
            " a_3x3 (Conv2D)                                                     1x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_1_Conv2d_1  (None, 2, 2, 256)            589824    ['Mixed_7a_Branch_1_Conv2d_0a_\n",
            " a_3x3 (Conv2D)                                                     1x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_2_Conv2d_1  (None, 2, 2, 256)            589824    ['Mixed_7a_Branch_2_Conv2d_0b_\n",
            " a_3x3 (Conv2D)                                                     3x3_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_0_Conv2d_1  (None, 2, 2, 384)            1152      ['Mixed_7a_Branch_0_Conv2d_1a_\n",
            " a_3x3_BatchNorm (BatchNorm                                         3x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_1_Conv2d_1  (None, 2, 2, 256)            768       ['Mixed_7a_Branch_1_Conv2d_1a_\n",
            " a_3x3_BatchNorm (BatchNorm                                         3x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_2_Conv2d_1  (None, 2, 2, 256)            768       ['Mixed_7a_Branch_2_Conv2d_1a_\n",
            " a_3x3_BatchNorm (BatchNorm                                         3x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_0_Conv2d_1  (None, 2, 2, 384)            1536      ['Mixed_7a_Branch_0_Conv2d_1a_\n",
            " a_3x3_Activation (PReLU)                                           3x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_1_Conv2d_1  (None, 2, 2, 256)            1024      ['Mixed_7a_Branch_1_Conv2d_1a_\n",
            " a_3x3_Activation (PReLU)                                           3x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_2_Conv2d_1  (None, 2, 2, 256)            1024      ['Mixed_7a_Branch_2_Conv2d_1a_\n",
            " a_3x3_Activation (PReLU)                                           3x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Mixed_7a_Branch_3_MaxPool_  (None, 2, 2, 896)            0         ['Block17_10_Activation[0][0]'\n",
            " 1a_3x3 (MaxPooling2D)                                              ]                             \n",
            "                                                                                                  \n",
            " Mixed_7a (Concatenate)      (None, 2, 2, 1792)           0         ['Mixed_7a_Branch_0_Conv2d_1a_\n",
            "                                                                    3x3_Activation[0][0]',        \n",
            "                                                                     'Mixed_7a_Branch_1_Conv2d_1a_\n",
            "                                                                    3x3_Activation[0][0]',        \n",
            "                                                                     'Mixed_7a_Branch_2_Conv2d_1a_\n",
            "                                                                    3x3_Activation[0][0]',        \n",
            "                                                                     'Mixed_7a_Branch_3_MaxPool_1a\n",
            "                                                                    _3x3[0][0]']                  \n",
            "                                                                                                  \n",
            " Block8_1_Branch_1_Conv2d_0  (None, 2, 2, 192)            344064    ['Mixed_7a[0][0]']            \n",
            " a_1x1 (Conv2D)                                                                                   \n",
            "                                                                                                  \n",
            " Block8_1_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_1_Branch_1_Conv2d_0a_\n",
            " a_1x1_BatchNorm (BatchNorm                                         1x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_1_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_1_Branch_1_Conv2d_0a_\n",
            " a_1x1_Activation (PReLU)                                           1x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_1_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_1_Branch_1_Conv2d_0a_\n",
            " b_1x3 (Conv2D)                                                     1x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_1_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_1_Branch_1_Conv2d_0b_\n",
            " b_1x3_BatchNorm (BatchNorm                                         1x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_1_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_1_Branch_1_Conv2d_0b_\n",
            " b_1x3_Activation (PReLU)                                           1x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_1_Branch_0_Conv2d_1  (None, 2, 2, 192)            344064    ['Mixed_7a[0][0]']            \n",
            " x1 (Conv2D)                                                                                      \n",
            "                                                                                                  \n",
            " Block8_1_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_1_Branch_1_Conv2d_0b_\n",
            " c_3x1 (Conv2D)                                                     1x3_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_1_Branch_0_Conv2d_1  (None, 2, 2, 192)            576       ['Block8_1_Branch_0_Conv2d_1x1\n",
            " x1_BatchNorm (BatchNormali                                         [0][0]']                      \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " Block8_1_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_1_Branch_1_Conv2d_0c_\n",
            " c_3x1_BatchNorm (BatchNorm                                         3x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_1_Branch_0_Conv2d_1  (None, 2, 2, 192)            768       ['Block8_1_Branch_0_Conv2d_1x1\n",
            " x1_Activation (PReLU)                                              _BatchNorm[0][0]']            \n",
            "                                                                                                  \n",
            " Block8_1_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_1_Branch_1_Conv2d_0c_\n",
            " c_3x1_Activation (PReLU)                                           3x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_1_Concatenate (Conc  (None, 2, 2, 384)            0         ['Block8_1_Branch_0_Conv2d_1x1\n",
            " atenate)                                                           _Activation[0][0]',           \n",
            "                                                                     'Block8_1_Branch_1_Conv2d_0c_\n",
            "                                                                    3x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_1_Conv2d_1x1 (Conv2  (None, 2, 2, 1792)           689920    ['Block8_1_Concatenate[0][0]']\n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)          (None, 2, 2, 1792)           0         ['Block8_1_Conv2d_1x1[0][0]'] \n",
            "                                                                                                  \n",
            " add_15 (Add)                (None, 2, 2, 1792)           0         ['Mixed_7a[0][0]',            \n",
            "                                                                     'lambda_15[0][0]']           \n",
            "                                                                                                  \n",
            " Block8_1_Activation (PReLU  (None, 2, 2, 1792)           7168      ['add_15[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " Block8_2_Branch_1_Conv2d_0  (None, 2, 2, 192)            344064    ['Block8_1_Activation[0][0]'] \n",
            " a_1x1 (Conv2D)                                                                                   \n",
            "                                                                                                  \n",
            " Block8_2_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_2_Branch_1_Conv2d_0a_\n",
            " a_1x1_BatchNorm (BatchNorm                                         1x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_2_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_2_Branch_1_Conv2d_0a_\n",
            " a_1x1_Activation (PReLU)                                           1x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_2_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_2_Branch_1_Conv2d_0a_\n",
            " b_1x3 (Conv2D)                                                     1x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_2_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_2_Branch_1_Conv2d_0b_\n",
            " b_1x3_BatchNorm (BatchNorm                                         1x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_2_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_2_Branch_1_Conv2d_0b_\n",
            " b_1x3_Activation (PReLU)                                           1x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_2_Branch_0_Conv2d_1  (None, 2, 2, 192)            344064    ['Block8_1_Activation[0][0]'] \n",
            " x1 (Conv2D)                                                                                      \n",
            "                                                                                                  \n",
            " Block8_2_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_2_Branch_1_Conv2d_0b_\n",
            " c_3x1 (Conv2D)                                                     1x3_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_2_Branch_0_Conv2d_1  (None, 2, 2, 192)            576       ['Block8_2_Branch_0_Conv2d_1x1\n",
            " x1_BatchNorm (BatchNormali                                         [0][0]']                      \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " Block8_2_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_2_Branch_1_Conv2d_0c_\n",
            " c_3x1_BatchNorm (BatchNorm                                         3x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_2_Branch_0_Conv2d_1  (None, 2, 2, 192)            768       ['Block8_2_Branch_0_Conv2d_1x1\n",
            " x1_Activation (PReLU)                                              _BatchNorm[0][0]']            \n",
            "                                                                                                  \n",
            " Block8_2_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_2_Branch_1_Conv2d_0c_\n",
            " c_3x1_Activation (PReLU)                                           3x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_2_Concatenate (Conc  (None, 2, 2, 384)            0         ['Block8_2_Branch_0_Conv2d_1x1\n",
            " atenate)                                                           _Activation[0][0]',           \n",
            "                                                                     'Block8_2_Branch_1_Conv2d_0c_\n",
            "                                                                    3x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_2_Conv2d_1x1 (Conv2  (None, 2, 2, 1792)           689920    ['Block8_2_Concatenate[0][0]']\n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " lambda_16 (Lambda)          (None, 2, 2, 1792)           0         ['Block8_2_Conv2d_1x1[0][0]'] \n",
            "                                                                                                  \n",
            " add_16 (Add)                (None, 2, 2, 1792)           0         ['Block8_1_Activation[0][0]', \n",
            "                                                                     'lambda_16[0][0]']           \n",
            "                                                                                                  \n",
            " Block8_2_Activation (PReLU  (None, 2, 2, 1792)           7168      ['add_16[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " Block8_3_Branch_1_Conv2d_0  (None, 2, 2, 192)            344064    ['Block8_2_Activation[0][0]'] \n",
            " a_1x1 (Conv2D)                                                                                   \n",
            "                                                                                                  \n",
            " Block8_3_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_3_Branch_1_Conv2d_0a_\n",
            " a_1x1_BatchNorm (BatchNorm                                         1x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_3_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_3_Branch_1_Conv2d_0a_\n",
            " a_1x1_Activation (PReLU)                                           1x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_3_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_3_Branch_1_Conv2d_0a_\n",
            " b_1x3 (Conv2D)                                                     1x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_3_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_3_Branch_1_Conv2d_0b_\n",
            " b_1x3_BatchNorm (BatchNorm                                         1x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_3_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_3_Branch_1_Conv2d_0b_\n",
            " b_1x3_Activation (PReLU)                                           1x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_3_Branch_0_Conv2d_1  (None, 2, 2, 192)            344064    ['Block8_2_Activation[0][0]'] \n",
            " x1 (Conv2D)                                                                                      \n",
            "                                                                                                  \n",
            " Block8_3_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_3_Branch_1_Conv2d_0b_\n",
            " c_3x1 (Conv2D)                                                     1x3_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_3_Branch_0_Conv2d_1  (None, 2, 2, 192)            576       ['Block8_3_Branch_0_Conv2d_1x1\n",
            " x1_BatchNorm (BatchNormali                                         [0][0]']                      \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " Block8_3_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_3_Branch_1_Conv2d_0c_\n",
            " c_3x1_BatchNorm (BatchNorm                                         3x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_3_Branch_0_Conv2d_1  (None, 2, 2, 192)            768       ['Block8_3_Branch_0_Conv2d_1x1\n",
            " x1_Activation (PReLU)                                              _BatchNorm[0][0]']            \n",
            "                                                                                                  \n",
            " Block8_3_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_3_Branch_1_Conv2d_0c_\n",
            " c_3x1_Activation (PReLU)                                           3x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_3_Concatenate (Conc  (None, 2, 2, 384)            0         ['Block8_3_Branch_0_Conv2d_1x1\n",
            " atenate)                                                           _Activation[0][0]',           \n",
            "                                                                     'Block8_3_Branch_1_Conv2d_0c_\n",
            "                                                                    3x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_3_Conv2d_1x1 (Conv2  (None, 2, 2, 1792)           689920    ['Block8_3_Concatenate[0][0]']\n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " lambda_17 (Lambda)          (None, 2, 2, 1792)           0         ['Block8_3_Conv2d_1x1[0][0]'] \n",
            "                                                                                                  \n",
            " add_17 (Add)                (None, 2, 2, 1792)           0         ['Block8_2_Activation[0][0]', \n",
            "                                                                     'lambda_17[0][0]']           \n",
            "                                                                                                  \n",
            " Block8_3_Activation (PReLU  (None, 2, 2, 1792)           7168      ['add_17[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " Block8_4_Branch_1_Conv2d_0  (None, 2, 2, 192)            344064    ['Block8_3_Activation[0][0]'] \n",
            " a_1x1 (Conv2D)                                                                                   \n",
            "                                                                                                  \n",
            " Block8_4_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_4_Branch_1_Conv2d_0a_\n",
            " a_1x1_BatchNorm (BatchNorm                                         1x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_4_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_4_Branch_1_Conv2d_0a_\n",
            " a_1x1_Activation (PReLU)                                           1x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_4_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_4_Branch_1_Conv2d_0a_\n",
            " b_1x3 (Conv2D)                                                     1x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_4_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_4_Branch_1_Conv2d_0b_\n",
            " b_1x3_BatchNorm (BatchNorm                                         1x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_4_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_4_Branch_1_Conv2d_0b_\n",
            " b_1x3_Activation (PReLU)                                           1x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_4_Branch_0_Conv2d_1  (None, 2, 2, 192)            344064    ['Block8_3_Activation[0][0]'] \n",
            " x1 (Conv2D)                                                                                      \n",
            "                                                                                                  \n",
            " Block8_4_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_4_Branch_1_Conv2d_0b_\n",
            " c_3x1 (Conv2D)                                                     1x3_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_4_Branch_0_Conv2d_1  (None, 2, 2, 192)            576       ['Block8_4_Branch_0_Conv2d_1x1\n",
            " x1_BatchNorm (BatchNormali                                         [0][0]']                      \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " Block8_4_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_4_Branch_1_Conv2d_0c_\n",
            " c_3x1_BatchNorm (BatchNorm                                         3x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_4_Branch_0_Conv2d_1  (None, 2, 2, 192)            768       ['Block8_4_Branch_0_Conv2d_1x1\n",
            " x1_Activation (PReLU)                                              _BatchNorm[0][0]']            \n",
            "                                                                                                  \n",
            " Block8_4_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_4_Branch_1_Conv2d_0c_\n",
            " c_3x1_Activation (PReLU)                                           3x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_4_Concatenate (Conc  (None, 2, 2, 384)            0         ['Block8_4_Branch_0_Conv2d_1x1\n",
            " atenate)                                                           _Activation[0][0]',           \n",
            "                                                                     'Block8_4_Branch_1_Conv2d_0c_\n",
            "                                                                    3x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_4_Conv2d_1x1 (Conv2  (None, 2, 2, 1792)           689920    ['Block8_4_Concatenate[0][0]']\n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " lambda_18 (Lambda)          (None, 2, 2, 1792)           0         ['Block8_4_Conv2d_1x1[0][0]'] \n",
            "                                                                                                  \n",
            " add_18 (Add)                (None, 2, 2, 1792)           0         ['Block8_3_Activation[0][0]', \n",
            "                                                                     'lambda_18[0][0]']           \n",
            "                                                                                                  \n",
            " Block8_4_Activation (PReLU  (None, 2, 2, 1792)           7168      ['add_18[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " Block8_5_Branch_1_Conv2d_0  (None, 2, 2, 192)            344064    ['Block8_4_Activation[0][0]'] \n",
            " a_1x1 (Conv2D)                                                                                   \n",
            "                                                                                                  \n",
            " Block8_5_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_5_Branch_1_Conv2d_0a_\n",
            " a_1x1_BatchNorm (BatchNorm                                         1x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_5_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_5_Branch_1_Conv2d_0a_\n",
            " a_1x1_Activation (PReLU)                                           1x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_5_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_5_Branch_1_Conv2d_0a_\n",
            " b_1x3 (Conv2D)                                                     1x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_5_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_5_Branch_1_Conv2d_0b_\n",
            " b_1x3_BatchNorm (BatchNorm                                         1x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_5_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_5_Branch_1_Conv2d_0b_\n",
            " b_1x3_Activation (PReLU)                                           1x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_5_Branch_0_Conv2d_1  (None, 2, 2, 192)            344064    ['Block8_4_Activation[0][0]'] \n",
            " x1 (Conv2D)                                                                                      \n",
            "                                                                                                  \n",
            " Block8_5_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_5_Branch_1_Conv2d_0b_\n",
            " c_3x1 (Conv2D)                                                     1x3_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_5_Branch_0_Conv2d_1  (None, 2, 2, 192)            576       ['Block8_5_Branch_0_Conv2d_1x1\n",
            " x1_BatchNorm (BatchNormali                                         [0][0]']                      \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " Block8_5_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_5_Branch_1_Conv2d_0c_\n",
            " c_3x1_BatchNorm (BatchNorm                                         3x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_5_Branch_0_Conv2d_1  (None, 2, 2, 192)            768       ['Block8_5_Branch_0_Conv2d_1x1\n",
            " x1_Activation (PReLU)                                              _BatchNorm[0][0]']            \n",
            "                                                                                                  \n",
            " Block8_5_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_5_Branch_1_Conv2d_0c_\n",
            " c_3x1_Activation (PReLU)                                           3x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_5_Concatenate (Conc  (None, 2, 2, 384)            0         ['Block8_5_Branch_0_Conv2d_1x1\n",
            " atenate)                                                           _Activation[0][0]',           \n",
            "                                                                     'Block8_5_Branch_1_Conv2d_0c_\n",
            "                                                                    3x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_5_Conv2d_1x1 (Conv2  (None, 2, 2, 1792)           689920    ['Block8_5_Concatenate[0][0]']\n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " lambda_19 (Lambda)          (None, 2, 2, 1792)           0         ['Block8_5_Conv2d_1x1[0][0]'] \n",
            "                                                                                                  \n",
            " add_19 (Add)                (None, 2, 2, 1792)           0         ['Block8_4_Activation[0][0]', \n",
            "                                                                     'lambda_19[0][0]']           \n",
            "                                                                                                  \n",
            " Block8_5_Activation (PReLU  (None, 2, 2, 1792)           7168      ['add_19[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " Block8_6_Branch_1_Conv2d_0  (None, 2, 2, 192)            344064    ['Block8_5_Activation[0][0]'] \n",
            " a_1x1 (Conv2D)                                                                                   \n",
            "                                                                                                  \n",
            " Block8_6_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_6_Branch_1_Conv2d_0a_\n",
            " a_1x1_BatchNorm (BatchNorm                                         1x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_6_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_6_Branch_1_Conv2d_0a_\n",
            " a_1x1_Activation (PReLU)                                           1x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_6_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_6_Branch_1_Conv2d_0a_\n",
            " b_1x3 (Conv2D)                                                     1x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_6_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_6_Branch_1_Conv2d_0b_\n",
            " b_1x3_BatchNorm (BatchNorm                                         1x3[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_6_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_6_Branch_1_Conv2d_0b_\n",
            " b_1x3_Activation (PReLU)                                           1x3_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_6_Branch_0_Conv2d_1  (None, 2, 2, 192)            344064    ['Block8_5_Activation[0][0]'] \n",
            " x1 (Conv2D)                                                                                      \n",
            "                                                                                                  \n",
            " Block8_6_Branch_1_Conv2d_0  (None, 2, 2, 192)            110592    ['Block8_6_Branch_1_Conv2d_0b_\n",
            " c_3x1 (Conv2D)                                                     1x3_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_6_Branch_0_Conv2d_1  (None, 2, 2, 192)            576       ['Block8_6_Branch_0_Conv2d_1x1\n",
            " x1_BatchNorm (BatchNormali                                         [0][0]']                      \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " Block8_6_Branch_1_Conv2d_0  (None, 2, 2, 192)            576       ['Block8_6_Branch_1_Conv2d_0c_\n",
            " c_3x1_BatchNorm (BatchNorm                                         3x1[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Block8_6_Branch_0_Conv2d_1  (None, 2, 2, 192)            768       ['Block8_6_Branch_0_Conv2d_1x1\n",
            " x1_Activation (PReLU)                                              _BatchNorm[0][0]']            \n",
            "                                                                                                  \n",
            " Block8_6_Branch_1_Conv2d_0  (None, 2, 2, 192)            768       ['Block8_6_Branch_1_Conv2d_0c_\n",
            " c_3x1_Activation (PReLU)                                           3x1_BatchNorm[0][0]']         \n",
            "                                                                                                  \n",
            " Block8_6_Concatenate (Conc  (None, 2, 2, 384)            0         ['Block8_6_Branch_0_Conv2d_1x1\n",
            " atenate)                                                           _Activation[0][0]',           \n",
            "                                                                     'Block8_6_Branch_1_Conv2d_0c_\n",
            "                                                                    3x1_Activation[0][0]']        \n",
            "                                                                                                  \n",
            " Block8_6_Conv2d_1x1 (Conv2  (None, 2, 2, 1792)           689920    ['Block8_6_Concatenate[0][0]']\n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " lambda_20 (Lambda)          (None, 2, 2, 1792)           0         ['Block8_6_Conv2d_1x1[0][0]'] \n",
            "                                                                                                  \n",
            " add_20 (Add)                (None, 2, 2, 1792)           0         ['Block8_5_Activation[0][0]', \n",
            "                                                                     'lambda_20[0][0]']           \n",
            "                                                                                                  \n",
            " global_average_pooling2d (  (None, 1792)                 0         ['add_20[0][0]']              \n",
            " GlobalAveragePooling2D)                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 1792)                 7168      ['global_average_pooling2d[0][\n",
            " Normalization)                                                     0]']                          \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 1792)                 0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " features_without_bn (Dense  (None, 512)                  918016    ['dropout[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 512)                  1536      ['features_without_bn[0][0]'] \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " arcfaceLayer (ArcFaceLayer  (None, None, 85742)          4389990   ['batch_normalization_1[0][0]'\n",
            " )                                                        4         , 'input_1[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 68736080 (262.21 MB)\n",
            "Trainable params: 68702896 (262.08 MB)\n",
            "Non-trainable params: 33184 (129.62 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ArcFaceModel/model.tf\n",
            "[*] THERE IS NO WEIGHT FILE FOR MODEL, INITIALIZING...\n",
            "[*] Possible maximum step: -2\n",
            "\n",
            "[*] TensorBoard initialized on classifier_tensorboard\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1722782648.350902    9890 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "[0] Loss: 46.051170349121094 || Reg Loss: 14.407293319702148 || Accuracy: %0.0 || LR: 0.004000000189989805\n",
            "[500] Loss: 38.47907257080078 || Reg Loss: 14.174782752990723 || Accuracy: %0.0002656250144354999 || LR: 0.004000000189989805\n",
            "[1000] Loss: 32.167362213134766 || Reg Loss: 13.863399505615234 || Accuracy: %0.001687500043772161 || LR: 0.004000000189989805\n",
            "[1500] Loss: 27.80853271484375 || Reg Loss: 13.563864707946777 || Accuracy: %0.005750000011175871 || LR: 0.004000000189989805\n",
            "[2000] Loss: 24.846641540527344 || Reg Loss: 13.275948524475098 || Accuracy: %0.013187499716877937 || LR: 0.004000000189989805\n",
            "[2500] Loss: 22.550617218017578 || Reg Loss: 12.997993469238281 || Accuracy: %0.02043749950826168 || LR: 0.004000000189989805\n",
            "[3000] Loss: 21.076045989990234 || Reg Loss: 12.729680061340332 || Accuracy: %0.027796875685453415 || LR: 0.004000000189989805\n",
            "[3500] Loss: 20.20612335205078 || Reg Loss: 12.468781471252441 || Accuracy: %0.03279687464237213 || LR: 0.004000000189989805\n",
            "[4000] Loss: 19.411771774291992 || Reg Loss: 12.215155601501465 || Accuracy: %0.04046874865889549 || LR: 0.004000000189989805\n",
            "[4500] Loss: 18.506853103637695 || Reg Loss: 11.962032318115234 || Accuracy: %0.044874999672174454 || LR: 0.004000000189989805\n",
            "[5000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[5000] Loss: 17.605009078979492 || Reg Loss: 11.71348762512207 || Accuracy: %0.04695312678813934 || LR: 0.004000000189989805\n",
            "[5500] Loss: 16.933292388916016 || Reg Loss: 11.473419189453125 || Accuracy: %0.049687501043081284 || LR: 0.004000000189989805\n",
            "[6000] Loss: 16.141529083251953 || Reg Loss: 11.242920875549316 || Accuracy: %0.06482812762260437 || LR: 0.004000000189989805\n",
            "[6500] Loss: 16.329856872558594 || Reg Loss: 11.020455360412598 || Accuracy: %0.058140624314546585 || LR: 0.004000000189989805\n",
            "[7000] Loss: 16.16044807434082 || Reg Loss: 10.804954528808594 || Accuracy: %0.058531250804662704 || LR: 0.004000000189989805\n",
            "[7500] Loss: 16.489704132080078 || Reg Loss: 10.623239517211914 || Accuracy: %0.07534375041723251 || LR: 0.004000000189989805\n",
            "[8000] Loss: 16.60114097595215 || Reg Loss: 10.432995796203613 || Accuracy: %0.07789062708616257 || LR: 0.004000000189989805\n",
            "[8500] Loss: 16.627256393432617 || Reg Loss: 10.247834205627441 || Accuracy: %0.08193749934434891 || LR: 0.004000000189989805\n",
            "[9000] Loss: 16.503374099731445 || Reg Loss: 10.067562103271484 || Accuracy: %0.08304687589406967 || LR: 0.004000000189989805\n",
            "[9500] Loss: 16.902402877807617 || Reg Loss: 9.895886421203613 || Accuracy: %0.07164062559604645 || LR: 0.004000000189989805\n",
            "[10000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[10000] Loss: 17.251741409301758 || Reg Loss: 9.735788345336914 || Accuracy: %0.06475000083446503 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [06:59<00:00,  1.79it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9751666666666667 || Best Threshold --> 1.198\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_10000.h5\n",
            "2024-08-04 15:16:07.426526: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 15:16:07.426584: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_10000.tflite\n",
            "2024-08-04 15:17:16.890444: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 15:17:16.890510: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_10000.tflite\n",
            "[*] Final model saved\n",
            "[10500] Loss: 17.05172348022461 || Reg Loss: 9.5799560546875 || Accuracy: %0.06692187488079071 || LR: 0.004000000189989805\n",
            "[11000] Loss: 16.87539291381836 || Reg Loss: 9.430964469909668 || Accuracy: %0.06595312803983688 || LR: 0.004000000189989805\n",
            "[11500] Loss: 16.996971130371094 || Reg Loss: 9.29061222076416 || Accuracy: %0.06599999964237213 || LR: 0.004000000189989805\n",
            "[12000] Loss: 17.56697654724121 || Reg Loss: 9.160155296325684 || Accuracy: %0.057359375059604645 || LR: 0.004000000189989805\n",
            "[12500] Loss: 16.65709686279297 || Reg Loss: 9.029640197753906 || Accuracy: %0.0657343715429306 || LR: 0.004000000189989805\n",
            "[13000] Loss: 15.140315055847168 || Reg Loss: 8.899054527282715 || Accuracy: %0.08840624988079071 || LR: 0.004000000189989805\n",
            "[13500] Loss: 14.676594734191895 || Reg Loss: 8.77830982208252 || Accuracy: %0.09359374642372131 || LR: 0.004000000189989805\n",
            "[14000] Loss: 14.767937660217285 || Reg Loss: 8.665428161621094 || Accuracy: %0.09014062583446503 || LR: 0.004000000189989805\n",
            "[14500] Loss: 14.715599060058594 || Reg Loss: 8.559233665466309 || Accuracy: %0.08896874636411667 || LR: 0.004000000189989805\n",
            "[15000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[15000] Loss: 14.835386276245117 || Reg Loss: 8.461004257202148 || Accuracy: %0.0898749977350235 || LR: 0.004000000189989805\n",
            "[15500] Loss: 14.489493370056152 || Reg Loss: 8.362071990966797 || Accuracy: %0.09785937517881393 || LR: 0.004000000189989805\n",
            "[16000] Loss: 13.978846549987793 || Reg Loss: 8.265481948852539 || Accuracy: %0.11232812702655792 || LR: 0.004000000189989805\n",
            "[16500] Loss: 14.033536911010742 || Reg Loss: 8.178235054016113 || Accuracy: %0.11090625077486038 || LR: 0.004000000189989805\n",
            "[17000] Loss: 14.168874740600586 || Reg Loss: 8.102455139160156 || Accuracy: %0.10387499630451202 || LR: 0.004000000189989805\n",
            "[17500] Loss: 14.163311958312988 || Reg Loss: 8.028583526611328 || Accuracy: %0.10239062458276749 || LR: 0.004000000189989805\n",
            "[18000] Loss: 14.104738235473633 || Reg Loss: 7.958602428436279 || Accuracy: %0.10257812589406967 || LR: 0.004000000189989805\n",
            "[18500] Loss: 13.784829139709473 || Reg Loss: 7.888319969177246 || Accuracy: %0.10821875184774399 || LR: 0.004000000189989805\n",
            "[19000] Loss: 13.790449142456055 || Reg Loss: 7.824419975280762 || Accuracy: %0.11031249910593033 || LR: 0.004000000189989805\n",
            "[19500] Loss: 13.68349552154541 || Reg Loss: 7.768850326538086 || Accuracy: %0.11503124982118607 || LR: 0.004000000189989805\n",
            "[20000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[20000] Loss: 13.980753898620605 || Reg Loss: 7.719388961791992 || Accuracy: %0.11345312744379044 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:10<00:00,  1.74it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9843333333333335 || Best Threshold --> 1.161\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_20000.h5\n",
            "2024-08-04 15:49:37.606061: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 15:49:37.606111: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_20000.tflite\n",
            "2024-08-04 15:50:47.602835: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 15:50:47.602903: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_20000.tflite\n",
            "[*] Final model saved\n",
            "[20500] Loss: 13.993657112121582 || Reg Loss: 7.670627117156982 || Accuracy: %0.10796874761581421 || LR: 0.004000000189989805\n",
            "[21000] Loss: 13.912355422973633 || Reg Loss: 7.6281890869140625 || Accuracy: %0.11128125339746475 || LR: 0.004000000189989805\n",
            "[21500] Loss: 13.526152610778809 || Reg Loss: 7.589804172515869 || Accuracy: %0.11968749761581421 || LR: 0.004000000189989805\n",
            "[22000] Loss: 13.449249267578125 || Reg Loss: 7.550378799438477 || Accuracy: %0.12028124928474426 || LR: 0.004000000189989805\n",
            "[22500] Loss: 13.364933967590332 || Reg Loss: 7.515106678009033 || Accuracy: %0.12215624749660492 || LR: 0.004000000189989805\n",
            "[23000] Loss: 13.08326530456543 || Reg Loss: 7.484403610229492 || Accuracy: %0.13139063119888306 || LR: 0.004000000189989805\n",
            "[23500] Loss: 13.207993507385254 || Reg Loss: 7.461685657501221 || Accuracy: %0.13120312988758087 || LR: 0.004000000189989805\n",
            "[24000] Loss: 13.148681640625 || Reg Loss: 7.441249847412109 || Accuracy: %0.12473437190055847 || LR: 0.004000000189989805\n",
            "[24500] Loss: 13.331205368041992 || Reg Loss: 7.430482864379883 || Accuracy: %0.12257812172174454 || LR: 0.004000000189989805\n",
            "[25000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[25000] Loss: 13.175739288330078 || Reg Loss: 7.420993328094482 || Accuracy: %0.12785936892032623 || LR: 0.004000000189989805\n",
            "[25500] Loss: 13.136608123779297 || Reg Loss: 7.409524917602539 || Accuracy: %0.12848436832427979 || LR: 0.004000000189989805\n",
            "[26000] Loss: 13.127469062805176 || Reg Loss: 7.403807163238525 || Accuracy: %0.1262812465429306 || LR: 0.004000000189989805\n",
            "[26500] Loss: 13.2875394821167 || Reg Loss: 7.399566650390625 || Accuracy: %0.1274687498807907 || LR: 0.004000000189989805\n",
            "[27000] Loss: 13.100775718688965 || Reg Loss: 7.399858474731445 || Accuracy: %0.12800000607967377 || LR: 0.004000000189989805\n",
            "[27500] Loss: 13.136656761169434 || Reg Loss: 7.40696907043457 || Accuracy: %0.13234375417232513 || LR: 0.004000000189989805\n",
            "[28000] Loss: 13.10356330871582 || Reg Loss: 7.413085460662842 || Accuracy: %0.13315625488758087 || LR: 0.004000000189989805\n",
            "[28500] Loss: 12.860188484191895 || Reg Loss: 7.420620918273926 || Accuracy: %0.13789062201976776 || LR: 0.004000000189989805\n",
            "[29000] Loss: 12.824296951293945 || Reg Loss: 7.4267168045043945 || Accuracy: %0.13971875607967377 || LR: 0.004000000189989805\n",
            "[29500] Loss: 12.815048217773438 || Reg Loss: 7.441802024841309 || Accuracy: %0.14032812416553497 || LR: 0.004000000189989805\n",
            "[30000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[30000] Loss: 12.891095161437988 || Reg Loss: 7.470602035522461 || Accuracy: %0.13948437571525574 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:10<00:00,  1.74it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9884999999999999 || Best Threshold --> 1.1099999999999999\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_30000.h5\n",
            "2024-08-04 16:23:08.717884: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 16:23:08.717955: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_30000.tflite\n",
            "2024-08-04 16:24:19.263245: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 16:24:19.263304: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_30000.tflite\n",
            "[*] Final model saved\n",
            "[30500] Loss: 12.955592155456543 || Reg Loss: 7.5053863525390625 || Accuracy: %0.1393750011920929 || LR: 0.004000000189989805\n",
            "[31000] Loss: 13.196043014526367 || Reg Loss: 7.5439534187316895 || Accuracy: %0.13307812809944153 || LR: 0.004000000189989805\n",
            "[31500] Loss: 13.257636070251465 || Reg Loss: 7.583416938781738 || Accuracy: %0.13362500071525574 || LR: 0.004000000189989805\n",
            "[32000] Loss: 13.283483505249023 || Reg Loss: 7.62722110748291 || Accuracy: %0.13264063000679016 || LR: 0.004000000189989805\n",
            "[32500] Loss: 13.037225723266602 || Reg Loss: 7.669473648071289 || Accuracy: %0.14182811975479126 || LR: 0.004000000189989805\n",
            "[33000] Loss: 12.847649574279785 || Reg Loss: 7.715553283691406 || Accuracy: %0.1412968784570694 || LR: 0.004000000189989805\n",
            "[33500] Loss: 12.779730796813965 || Reg Loss: 7.761472702026367 || Accuracy: %0.1456875056028366 || LR: 0.004000000189989805\n",
            "[34000] Loss: 13.089598655700684 || Reg Loss: 7.823977947235107 || Accuracy: %0.1400781273841858 || LR: 0.004000000189989805\n",
            "[34500] Loss: 13.098031997680664 || Reg Loss: 7.88694953918457 || Accuracy: %0.1315937489271164 || LR: 0.004000000189989805\n",
            "[35000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[35000] Loss: 13.245577812194824 || Reg Loss: 7.963672637939453 || Accuracy: %0.12745311856269836 || LR: 0.004000000189989805\n",
            "[35500] Loss: 13.100696563720703 || Reg Loss: 8.042202949523926 || Accuracy: %0.12904687225818634 || LR: 0.004000000189989805\n",
            "[36000] Loss: 13.126924514770508 || Reg Loss: 8.136655807495117 || Accuracy: %0.13490624725818634 || LR: 0.004000000189989805\n",
            "[36500] Loss: 13.471023559570312 || Reg Loss: 8.229913711547852 || Accuracy: %0.1267968714237213 || LR: 0.004000000189989805\n",
            "[37000] Loss: 13.561840057373047 || Reg Loss: 8.33416748046875 || Accuracy: %0.1224687471985817 || LR: 0.004000000189989805\n",
            "[37500] Loss: 13.596046447753906 || Reg Loss: 8.448959350585938 || Accuracy: %0.12381249666213989 || LR: 0.004000000189989805\n",
            "[38000] Loss: 13.491278648376465 || Reg Loss: 8.551788330078125 || Accuracy: %0.12262500077486038 || LR: 0.004000000189989805\n",
            "[38500] Loss: 13.40294361114502 || Reg Loss: 8.679771423339844 || Accuracy: %0.1275624930858612 || LR: 0.004000000189989805\n",
            "[39000] Loss: 13.371407508850098 || Reg Loss: 8.813827514648438 || Accuracy: %0.12968750298023224 || LR: 0.004000000189989805\n",
            "[39500] Loss: 13.285649299621582 || Reg Loss: 8.94936752319336 || Accuracy: %0.12848436832427979 || LR: 0.004000000189989805\n",
            "[40000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[40000] Loss: 13.403732299804688 || Reg Loss: 9.087709426879883 || Accuracy: %0.12607812881469727 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:08<00:00,  1.75it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9870000000000001 || Best Threshold --> 1.1180000000000003\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_40000.h5\n",
            "2024-08-04 16:56:36.398664: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 16:56:36.398719: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_40000.tflite\n",
            "2024-08-04 16:57:48.566751: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 16:57:48.566803: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_40000.tflite\n",
            "[*] Final model saved\n",
            "[40500] Loss: 13.618882179260254 || Reg Loss: 9.235858917236328 || Accuracy: %0.1185312494635582 || LR: 0.004000000189989805\n",
            "[41000] Loss: 13.519319534301758 || Reg Loss: 9.374244689941406 || Accuracy: %0.1171249970793724 || LR: 0.004000000189989805\n",
            "[41500] Loss: 13.70287036895752 || Reg Loss: 9.547361373901367 || Accuracy: %0.11714062839746475 || LR: 0.004000000189989805\n",
            "[42000] Loss: 13.909165382385254 || Reg Loss: 9.726359367370605 || Accuracy: %0.11228124797344208 || LR: 0.004000000189989805\n",
            "[42500] Loss: 13.724533081054688 || Reg Loss: 9.904214859008789 || Accuracy: %0.11496874690055847 || LR: 0.004000000189989805\n",
            "[43000] Loss: 13.486759185791016 || Reg Loss: 10.0982666015625 || Accuracy: %0.11979687213897705 || LR: 0.004000000189989805\n",
            "[43500] Loss: 13.694198608398438 || Reg Loss: 10.310474395751953 || Accuracy: %0.11612500250339508 || LR: 0.004000000189989805\n",
            "[44000] Loss: 13.590619087219238 || Reg Loss: 10.516549110412598 || Accuracy: %0.1158749982714653 || LR: 0.004000000189989805\n",
            "[44500] Loss: 13.872697830200195 || Reg Loss: 10.735334396362305 || Accuracy: %0.11332812160253525 || LR: 0.004000000189989805\n",
            "[45000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[45000] Loss: 14.105938911437988 || Reg Loss: 10.972834587097168 || Accuracy: %0.10814062505960464 || LR: 0.004000000189989805\n",
            "[45500] Loss: 11.905370712280273 || Reg Loss: 10.73291301727295 || Accuracy: %0.16978125274181366 || LR: 0.004000000189989805\n",
            "[46000] Loss: 13.104792594909668 || Reg Loss: 10.605119705200195 || Accuracy: %0.13457812368869781 || LR: 0.004000000189989805\n",
            "[46500] Loss: 11.689929008483887 || Reg Loss: 10.42803955078125 || Accuracy: %0.16968749463558197 || LR: 0.004000000189989805\n",
            "[47000] Loss: 11.688745498657227 || Reg Loss: 10.261942863464355 || Accuracy: %0.16917186975479126 || LR: 0.004000000189989805\n",
            "[47500] Loss: 11.706876754760742 || Reg Loss: 10.103449821472168 || Accuracy: %0.16945312917232513 || LR: 0.004000000189989805\n",
            "[48000] Loss: 11.290304183959961 || Reg Loss: 9.944587707519531 || Accuracy: %0.1803593784570694 || LR: 0.004000000189989805\n",
            "[48500] Loss: 11.322388648986816 || Reg Loss: 9.795737266540527 || Accuracy: %0.17835937440395355 || LR: 0.004000000189989805\n",
            "[49000] Loss: 11.455052375793457 || Reg Loss: 9.650886535644531 || Accuracy: %0.17854687571525574 || LR: 0.004000000189989805\n",
            "[49500] Loss: 11.48156452178955 || Reg Loss: 9.515450477600098 || Accuracy: %0.17578125 || LR: 0.004000000189989805\n",
            "[50000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[50000] Loss: 11.299128532409668 || Reg Loss: 9.374367713928223 || Accuracy: %0.17995312809944153 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:09<00:00,  1.75it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9871666666666666 || Best Threshold --> 1.153\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_50000.h5\n",
            "2024-08-04 17:30:01.908074: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 17:30:01.908128: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_50000.tflite\n",
            "2024-08-04 17:31:14.478498: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 17:31:14.478556: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_50000.tflite\n",
            "[*] Final model saved\n",
            "[50500] Loss: 10.73400592803955 || Reg Loss: 9.232616424560547 || Accuracy: %0.18645311892032623 || LR: 0.004000000189989805\n",
            "[51000] Loss: 10.68885326385498 || Reg Loss: 9.101256370544434 || Accuracy: %0.18879687786102295 || LR: 0.004000000189989805\n",
            "[51500] Loss: 10.435894012451172 || Reg Loss: 8.974113464355469 || Accuracy: %0.20310936868190765 || LR: 0.004000000189989805\n",
            "[52000] Loss: 10.922911643981934 || Reg Loss: 8.85362434387207 || Accuracy: %0.18745312094688416 || LR: 0.004000000189989805\n",
            "[52500] Loss: 11.066835403442383 || Reg Loss: 8.74561882019043 || Accuracy: %0.1792031228542328 || LR: 0.004000000189989805\n",
            "[53000] Loss: 11.324748039245605 || Reg Loss: 8.67532730102539 || Accuracy: %0.19768750667572021 || LR: 0.004000000189989805\n",
            "[53500] Loss: 11.602632522583008 || Reg Loss: 8.577014923095703 || Accuracy: %0.18965624272823334 || LR: 0.004000000189989805\n",
            "[54000] Loss: 11.871434211730957 || Reg Loss: 8.480070114135742 || Accuracy: %0.18614062666893005 || LR: 0.004000000189989805\n",
            "[54500] Loss: 12.025548934936523 || Reg Loss: 8.382831573486328 || Accuracy: %0.17745313048362732 || LR: 0.004000000189989805\n",
            "[55000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[55000] Loss: 12.43854808807373 || Reg Loss: 8.292933464050293 || Accuracy: %0.16459375619888306 || LR: 0.004000000189989805\n",
            "[55500] Loss: 12.911832809448242 || Reg Loss: 8.210209846496582 || Accuracy: %0.1485312432050705 || LR: 0.004000000189989805\n",
            "[56000] Loss: 12.76699161529541 || Reg Loss: 8.127120018005371 || Accuracy: %0.1511562466621399 || LR: 0.004000000189989805\n",
            "[56500] Loss: 12.753178596496582 || Reg Loss: 8.05039119720459 || Accuracy: %0.1466406285762787 || LR: 0.004000000189989805\n",
            "[57000] Loss: 13.081514358520508 || Reg Loss: 7.980167388916016 || Accuracy: %0.14579688012599945 || LR: 0.004000000189989805\n",
            "[57500] Loss: 13.740436553955078 || Reg Loss: 7.916156768798828 || Accuracy: %0.1296718716621399 || LR: 0.004000000189989805\n",
            "[58000] Loss: 12.847400665283203 || Reg Loss: 7.846181392669678 || Accuracy: %0.14857812225818634 || LR: 0.004000000189989805\n",
            "[58500] Loss: 11.409961700439453 || Reg Loss: 7.765721797943115 || Accuracy: %0.1880156248807907 || LR: 0.004000000189989805\n",
            "[59000] Loss: 11.056306838989258 || Reg Loss: 7.690587997436523 || Accuracy: %0.19621874392032623 || LR: 0.004000000189989805\n",
            "[59500] Loss: 11.155313491821289 || Reg Loss: 7.622984886169434 || Accuracy: %0.1893593817949295 || LR: 0.004000000189989805\n",
            "[60000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[60000] Loss: 11.187200546264648 || Reg Loss: 7.558229446411133 || Accuracy: %0.18720312416553497 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:10<00:00,  1.74it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9889999999999999 || Best Threshold --> 1.1789999999999998\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_60000.h5\n",
            "2024-08-04 18:03:28.423348: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 18:03:28.423406: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_60000.tflite\n",
            "2024-08-04 18:04:40.355441: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 18:04:40.355503: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_60000.tflite\n",
            "[*] Final model saved\n",
            "[60500] Loss: 11.346182823181152 || Reg Loss: 7.498711585998535 || Accuracy: %0.18462499976158142 || LR: 0.004000000189989805\n",
            "[61000] Loss: 11.175637245178223 || Reg Loss: 7.436548709869385 || Accuracy: %0.19098436832427979 || LR: 0.004000000189989805\n",
            "[61500] Loss: 10.811544418334961 || Reg Loss: 7.37160587310791 || Accuracy: %0.20681250095367432 || LR: 0.004000000189989805\n",
            "[62000] Loss: 10.87397289276123 || Reg Loss: 7.310930252075195 || Accuracy: %0.2062968760728836 || LR: 0.004000000189989805\n",
            "[62500] Loss: 10.97199535369873 || Reg Loss: 7.258906841278076 || Accuracy: %0.19628125429153442 || LR: 0.004000000189989805\n",
            "[63000] Loss: 11.078901290893555 || Reg Loss: 7.212206840515137 || Accuracy: %0.19445312023162842 || LR: 0.004000000189989805\n",
            "[63500] Loss: 11.077037811279297 || Reg Loss: 7.165538787841797 || Accuracy: %0.1901250034570694 || LR: 0.004000000189989805\n",
            "[64000] Loss: 10.79936408996582 || Reg Loss: 7.11625862121582 || Accuracy: %0.20026563107967377 || LR: 0.004000000189989805\n",
            "[64500] Loss: 10.95212459564209 || Reg Loss: 7.071486473083496 || Accuracy: %0.1965624988079071 || LR: 0.004000000189989805\n",
            "[65000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[65000] Loss: 10.769246101379395 || Reg Loss: 7.030025482177734 || Accuracy: %0.20256249606609344 || LR: 0.004000000189989805\n",
            "[65500] Loss: 11.060338020324707 || Reg Loss: 6.989072799682617 || Accuracy: %0.19940625131130219 || LR: 0.004000000189989805\n",
            "[66000] Loss: 11.26905345916748 || Reg Loss: 6.9556708335876465 || Accuracy: %0.18857812881469727 || LR: 0.004000000189989805\n",
            "[66500] Loss: 11.143491744995117 || Reg Loss: 6.918673515319824 || Accuracy: %0.19349999725818634 || LR: 0.004000000189989805\n",
            "[67000] Loss: 10.871395111083984 || Reg Loss: 6.880179405212402 || Accuracy: %0.20039062201976776 || LR: 0.004000000189989805\n",
            "[67500] Loss: 10.776897430419922 || Reg Loss: 6.839837074279785 || Accuracy: %0.20217187702655792 || LR: 0.004000000189989805\n",
            "[68000] Loss: 10.850903511047363 || Reg Loss: 6.805262565612793 || Accuracy: %0.19859375059604645 || LR: 0.004000000189989805\n",
            "[68500] Loss: 10.539873123168945 || Reg Loss: 6.769421577453613 || Accuracy: %0.21181249618530273 || LR: 0.004000000189989805\n",
            "[69000] Loss: 10.702356338500977 || Reg Loss: 6.7357306480407715 || Accuracy: %0.20593750476837158 || LR: 0.004000000189989805\n",
            "[69500] Loss: 10.679532051086426 || Reg Loss: 6.703681945800781 || Accuracy: %0.20374999940395355 || LR: 0.004000000189989805\n",
            "[70000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[70000] Loss: 10.851889610290527 || Reg Loss: 6.676467418670654 || Accuracy: %0.20096875727176666 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:08<00:00,  1.75it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9906666666666666 || Best Threshold --> 1.167\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_70000.h5\n",
            "2024-08-04 18:36:51.215525: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 18:36:51.215575: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_70000.tflite\n",
            "2024-08-04 18:38:01.902759: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 18:38:01.902822: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_70000.tflite\n",
            "[*] Final model saved\n",
            "[70500] Loss: 10.756987571716309 || Reg Loss: 6.64951229095459 || Accuracy: %0.20100000500679016 || LR: 0.004000000189989805\n",
            "[71000] Loss: 10.725890159606934 || Reg Loss: 6.6216325759887695 || Accuracy: %0.19837500154972076 || LR: 0.004000000189989805\n",
            "[71500] Loss: 10.748231887817383 || Reg Loss: 6.596765041351318 || Accuracy: %0.2026718705892563 || LR: 0.004000000189989805\n",
            "[72000] Loss: 10.855552673339844 || Reg Loss: 6.57209587097168 || Accuracy: %0.19807812571525574 || LR: 0.004000000189989805\n",
            "[72500] Loss: 10.747751235961914 || Reg Loss: 6.547091484069824 || Accuracy: %0.20018750429153442 || LR: 0.004000000189989805\n",
            "[73000] Loss: 10.78217887878418 || Reg Loss: 6.5241241455078125 || Accuracy: %0.20285937190055847 || LR: 0.004000000189989805\n",
            "[73500] Loss: 10.717218399047852 || Reg Loss: 6.500696182250977 || Accuracy: %0.20465624332427979 || LR: 0.004000000189989805\n",
            "[74000] Loss: 10.562431335449219 || Reg Loss: 6.476301193237305 || Accuracy: %0.2104843705892563 || LR: 0.004000000189989805\n",
            "[74500] Loss: 10.468491554260254 || Reg Loss: 6.450186729431152 || Accuracy: %0.21464063227176666 || LR: 0.004000000189989805\n",
            "[75000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[75000] Loss: 10.519200325012207 || Reg Loss: 6.42751407623291 || Accuracy: %0.20989061892032623 || LR: 0.004000000189989805\n",
            "[75500] Loss: 10.547478675842285 || Reg Loss: 6.405311584472656 || Accuracy: %0.20896874368190765 || LR: 0.004000000189989805\n",
            "[76000] Loss: 10.507680892944336 || Reg Loss: 6.3837056159973145 || Accuracy: %0.2142031192779541 || LR: 0.004000000189989805\n",
            "[76500] Loss: 10.737269401550293 || Reg Loss: 6.365413665771484 || Accuracy: %0.20270311832427979 || LR: 0.004000000189989805\n",
            "[77000] Loss: 10.799158096313477 || Reg Loss: 6.346931457519531 || Accuracy: %0.20228125154972076 || LR: 0.004000000189989805\n",
            "[77500] Loss: 10.770238876342773 || Reg Loss: 6.325476169586182 || Accuracy: %0.2075781226158142 || LR: 0.004000000189989805\n",
            "[78000] Loss: 10.617023468017578 || Reg Loss: 6.303650856018066 || Accuracy: %0.21159374713897705 || LR: 0.004000000189989805\n",
            "[78500] Loss: 10.281146049499512 || Reg Loss: 6.281994819641113 || Accuracy: %0.21498437225818634 || LR: 0.004000000189989805\n",
            "[79000] Loss: 10.236748695373535 || Reg Loss: 6.259861946105957 || Accuracy: %0.22042188048362732 || LR: 0.004000000189989805\n",
            "[79500] Loss: 10.44487190246582 || Reg Loss: 6.240679740905762 || Accuracy: %0.21268750727176666 || LR: 0.004000000189989805\n",
            "[80000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[80000] Loss: 10.596513748168945 || Reg Loss: 6.224701881408691 || Accuracy: %0.20331250131130219 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:09<00:00,  1.74it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9894999999999999 || Best Threshold --> 1.1190000000000002\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_80000.h5\n",
            "2024-08-04 19:10:21.213953: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 19:10:21.214016: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_80000.tflite\n",
            "2024-08-04 19:11:32.291666: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 19:11:32.291717: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_80000.tflite\n",
            "[*] Final model saved\n",
            "[80500] Loss: 10.600468635559082 || Reg Loss: 6.210002899169922 || Accuracy: %0.20215624570846558 || LR: 0.004000000189989805\n",
            "[81000] Loss: 10.310356140136719 || Reg Loss: 6.192999839782715 || Accuracy: %0.20932812988758087 || LR: 0.004000000189989805\n",
            "[81500] Loss: 10.34080696105957 || Reg Loss: 6.175703048706055 || Accuracy: %0.21389062702655792 || LR: 0.004000000189989805\n",
            "[82000] Loss: 10.677135467529297 || Reg Loss: 6.162104606628418 || Accuracy: %0.2031562477350235 || LR: 0.004000000189989805\n",
            "[82500] Loss: 10.526712417602539 || Reg Loss: 6.145033836364746 || Accuracy: %0.20800000429153442 || LR: 0.004000000189989805\n",
            "[83000] Loss: 10.679539680480957 || Reg Loss: 6.1307148933410645 || Accuracy: %0.20057812333106995 || LR: 0.004000000189989805\n",
            "[83500] Loss: 10.449188232421875 || Reg Loss: 6.114344120025635 || Accuracy: %0.20740625262260437 || LR: 0.004000000189989805\n",
            "[84000] Loss: 10.296669960021973 || Reg Loss: 6.098586082458496 || Accuracy: %0.21590624749660492 || LR: 0.004000000189989805\n",
            "[84500] Loss: 10.198570251464844 || Reg Loss: 6.081902503967285 || Accuracy: %0.21898438036441803 || LR: 0.004000000189989805\n",
            "[85000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[85000] Loss: 10.144043922424316 || Reg Loss: 6.065141677856445 || Accuracy: %0.21893750131130219 || LR: 0.004000000189989805\n",
            "[85500] Loss: 10.122099876403809 || Reg Loss: 6.049952507019043 || Accuracy: %0.21537500619888306 || LR: 0.004000000189989805\n",
            "[86000] Loss: 10.189187049865723 || Reg Loss: 6.03580904006958 || Accuracy: %0.21484375 || LR: 0.004000000189989805\n",
            "[86500] Loss: 10.087925910949707 || Reg Loss: 6.0215911865234375 || Accuracy: %0.21403124928474426 || LR: 0.004000000189989805\n",
            "[87000] Loss: 10.20284366607666 || Reg Loss: 6.008764743804932 || Accuracy: %0.2134062498807907 || LR: 0.004000000189989805\n",
            "[87500] Loss: 10.407817840576172 || Reg Loss: 5.995096206665039 || Accuracy: %0.2094999998807907 || LR: 0.004000000189989805\n",
            "[88000] Loss: 10.009140014648438 || Reg Loss: 5.9788923263549805 || Accuracy: %0.22171874344348907 || LR: 0.004000000189989805\n",
            "[88500] Loss: 9.847332000732422 || Reg Loss: 5.9619550704956055 || Accuracy: %0.22474999725818634 || LR: 0.004000000189989805\n",
            "[89000] Loss: 9.76733112335205 || Reg Loss: 5.94468879699707 || Accuracy: %0.22782813012599945 || LR: 0.004000000189989805\n",
            "[89500] Loss: 9.695244789123535 || Reg Loss: 5.92787504196167 || Accuracy: %0.23089063167572021 || LR: 0.004000000189989805\n",
            "[90000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[90000] Loss: 9.805038452148438 || Reg Loss: 5.911471843719482 || Accuracy: %0.22712500393390656 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:09<00:00,  1.75it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9911666666666668 || Best Threshold --> 1.1200000000000003\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_90000.h5\n",
            "2024-08-04 19:43:50.292603: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 19:43:50.292659: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_90000.tflite\n",
            "2024-08-04 19:45:02.198763: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 19:45:02.198818: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_90000.tflite\n",
            "[*] Final model saved\n",
            "[90500] Loss: 9.787027359008789 || Reg Loss: 5.894060134887695 || Accuracy: %0.2280624955892563 || LR: 0.004000000189989805\n",
            "[91000] Loss: 8.950427055358887 || Reg Loss: 5.865179061889648 || Accuracy: %0.28004688024520874 || LR: 0.004000000189989805\n",
            "[91500] Loss: 10.61727237701416 || Reg Loss: 5.904784202575684 || Accuracy: %0.21617187559604645 || LR: 0.004000000189989805\n",
            "[92000] Loss: 10.000354766845703 || Reg Loss: 5.912101745605469 || Accuracy: %0.23076562583446503 || LR: 0.004000000189989805\n",
            "[92500] Loss: 10.070184707641602 || Reg Loss: 5.921021938323975 || Accuracy: %0.2213750034570694 || LR: 0.004000000189989805\n",
            "[93000] Loss: 10.238378524780273 || Reg Loss: 5.927268981933594 || Accuracy: %0.2207343727350235 || LR: 0.004000000189989805\n",
            "[93500] Loss: 10.003292083740234 || Reg Loss: 5.9331865310668945 || Accuracy: %0.22292187809944153 || LR: 0.004000000189989805\n",
            "[94000] Loss: 10.000595092773438 || Reg Loss: 5.936662673950195 || Accuracy: %0.2252187430858612 || LR: 0.004000000189989805\n",
            "[94500] Loss: 10.300590515136719 || Reg Loss: 5.945955753326416 || Accuracy: %0.21585936844348907 || LR: 0.004000000189989805\n",
            "[95000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[95000] Loss: 10.274576187133789 || Reg Loss: 5.957139015197754 || Accuracy: %0.21171875298023224 || LR: 0.004000000189989805\n",
            "[95500] Loss: 10.101936340332031 || Reg Loss: 5.955569267272949 || Accuracy: %0.22062499821186066 || LR: 0.004000000189989805\n",
            "[96000] Loss: 9.538655281066895 || Reg Loss: 5.950836181640625 || Accuracy: %0.22898437082767487 || LR: 0.004000000189989805\n",
            "[96500] Loss: 9.485674858093262 || Reg Loss: 5.948843955993652 || Accuracy: %0.2316250056028366 || LR: 0.004000000189989805\n",
            "[97000] Loss: 9.370001792907715 || Reg Loss: 5.949504852294922 || Accuracy: %0.24582812190055847 || LR: 0.004000000189989805\n",
            "[97500] Loss: 9.89307975769043 || Reg Loss: 5.9538164138793945 || Accuracy: %0.22407811880111694 || LR: 0.004000000189989805\n",
            "[98000] Loss: 9.920737266540527 || Reg Loss: 5.965617656707764 || Accuracy: %0.2199062556028366 || LR: 0.004000000189989805\n",
            "[98500] Loss: 10.205910682678223 || Reg Loss: 6.0042524337768555 || Accuracy: %0.2417343705892563 || LR: 0.004000000189989805\n",
            "[99000] Loss: 10.513761520385742 || Reg Loss: 6.014286518096924 || Accuracy: %0.22892187535762787 || LR: 0.004000000189989805\n",
            "[99500] Loss: 10.846181869506836 || Reg Loss: 6.022356033325195 || Accuracy: %0.22223436832427979 || LR: 0.004000000189989805\n",
            "[100000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[100000] Loss: 11.106636047363281 || Reg Loss: 6.028311729431152 || Accuracy: %0.21112500131130219 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:09<00:00,  1.75it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9865 || Best Threshold --> 1.1430000000000002\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_100000.h5\n",
            "2024-08-04 20:17:20.752833: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 20:17:20.752893: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_100000.tflite\n",
            "2024-08-04 20:18:33.206536: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 20:18:33.206596: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_100000.tflite\n",
            "[*] Final model saved\n",
            "[100500] Loss: 11.458455085754395 || Reg Loss: 6.03723669052124 || Accuracy: %0.1954687535762787 || LR: 0.004000000189989805\n",
            "[101000] Loss: 11.930984497070312 || Reg Loss: 6.049572944641113 || Accuracy: %0.179625004529953 || LR: 0.004000000189989805\n",
            "[101500] Loss: 11.91154670715332 || Reg Loss: 6.058932304382324 || Accuracy: %0.17839062213897705 || LR: 0.004000000189989805\n",
            "[102000] Loss: 11.75656795501709 || Reg Loss: 6.0685930252075195 || Accuracy: %0.17998437583446503 || LR: 0.004000000189989805\n",
            "[102500] Loss: 12.159027099609375 || Reg Loss: 6.082308292388916 || Accuracy: %0.1770312488079071 || LR: 0.004000000189989805\n",
            "[103000] Loss: 12.83149242401123 || Reg Loss: 6.09978723526001 || Accuracy: %0.15626563131809235 || LR: 0.004000000189989805\n",
            "[103500] Loss: 11.972952842712402 || Reg Loss: 6.108114719390869 || Accuracy: %0.17496874928474426 || LR: 0.004000000189989805\n",
            "[104000] Loss: 10.600469589233398 || Reg Loss: 6.101253509521484 || Accuracy: %0.22160936892032623 || LR: 0.004000000189989805\n",
            "[104500] Loss: 10.261133193969727 || Reg Loss: 6.096325874328613 || Accuracy: %0.2317499965429306 || LR: 0.004000000189989805\n",
            "[105000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[105000] Loss: 10.353178024291992 || Reg Loss: 6.096750736236572 || Accuracy: %0.22156250476837158 || LR: 0.004000000189989805\n",
            "[105500] Loss: 10.302099227905273 || Reg Loss: 6.099778175354004 || Accuracy: %0.22282811999320984 || LR: 0.004000000189989805\n",
            "[106000] Loss: 10.489985466003418 || Reg Loss: 6.102665901184082 || Accuracy: %0.22104687988758087 || LR: 0.004000000189989805\n",
            "[106500] Loss: 10.283855438232422 || Reg Loss: 6.1006317138671875 || Accuracy: %0.22318750619888306 || LR: 0.004000000189989805\n",
            "[107000] Loss: 9.939443588256836 || Reg Loss: 6.092466354370117 || Accuracy: %0.2428281307220459 || LR: 0.004000000189989805\n",
            "[107500] Loss: 10.063642501831055 || Reg Loss: 6.090353965759277 || Accuracy: %0.23832812905311584 || LR: 0.004000000189989805\n",
            "[108000] Loss: 10.224058151245117 || Reg Loss: 6.092916488647461 || Accuracy: %0.22876562178134918 || LR: 0.004000000189989805\n",
            "[108500] Loss: 10.298867225646973 || Reg Loss: 6.0974531173706055 || Accuracy: %0.22685937583446503 || LR: 0.004000000189989805\n",
            "[109000] Loss: 10.261895179748535 || Reg Loss: 6.099357604980469 || Accuracy: %0.2253749966621399 || LR: 0.004000000189989805\n",
            "[109500] Loss: 10.119884490966797 || Reg Loss: 6.099884033203125 || Accuracy: %0.22943750023841858 || LR: 0.004000000189989805\n",
            "[110000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[110000] Loss: 10.148818016052246 || Reg Loss: 6.102469444274902 || Accuracy: %0.230406254529953 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:09<00:00,  1.74it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9908333333333333 || Best Threshold --> 1.161\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_110000.h5\n",
            "2024-08-04 20:50:49.121822: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 20:50:49.121889: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_110000.tflite\n",
            "2024-08-04 20:52:00.291025: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 20:52:00.291076: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_110000.tflite\n",
            "[*] Final model saved\n",
            "[110500] Loss: 10.027389526367188 || Reg Loss: 6.1069793701171875 || Accuracy: %0.2388906180858612 || LR: 0.004000000189989805\n",
            "[111000] Loss: 10.41270637512207 || Reg Loss: 6.11160945892334 || Accuracy: %0.22553125023841858 || LR: 0.004000000189989805\n",
            "[111500] Loss: 10.536016464233398 || Reg Loss: 6.119315147399902 || Accuracy: %0.21907812356948853 || LR: 0.004000000189989805\n",
            "[112000] Loss: 10.443663597106934 || Reg Loss: 6.121790885925293 || Accuracy: %0.22167187929153442 || LR: 0.004000000189989805\n",
            "[112500] Loss: 10.190893173217773 || Reg Loss: 6.123233795166016 || Accuracy: %0.23082812130451202 || LR: 0.004000000189989805\n",
            "[113000] Loss: 10.179807662963867 || Reg Loss: 6.125115394592285 || Accuracy: %0.22971874475479126 || LR: 0.004000000189989805\n",
            "[113500] Loss: 10.0817232131958 || Reg Loss: 6.126495361328125 || Accuracy: %0.23417188227176666 || LR: 0.004000000189989805\n",
            "[114000] Loss: 9.882757186889648 || Reg Loss: 6.125802993774414 || Accuracy: %0.24185937643051147 || LR: 0.004000000189989805\n",
            "[114500] Loss: 10.002493858337402 || Reg Loss: 6.126705169677734 || Accuracy: %0.24051561951637268 || LR: 0.004000000189989805\n",
            "[115000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[115000] Loss: 10.16322135925293 || Reg Loss: 6.129046440124512 || Accuracy: %0.2286093682050705 || LR: 0.004000000189989805\n",
            "[115500] Loss: 10.166452407836914 || Reg Loss: 6.136503219604492 || Accuracy: %0.23045311868190765 || LR: 0.004000000189989805\n",
            "[116000] Loss: 10.121623992919922 || Reg Loss: 6.142876625061035 || Accuracy: %0.23592187464237213 || LR: 0.004000000189989805\n",
            "[116500] Loss: 10.177164077758789 || Reg Loss: 6.147900581359863 || Accuracy: %0.22873437404632568 || LR: 0.004000000189989805\n",
            "[117000] Loss: 10.080872535705566 || Reg Loss: 6.150084495544434 || Accuracy: %0.23164062201976776 || LR: 0.004000000189989805\n",
            "[117500] Loss: 10.19058609008789 || Reg Loss: 6.153646469116211 || Accuracy: %0.232359379529953 || LR: 0.004000000189989805\n",
            "[118000] Loss: 10.117768287658691 || Reg Loss: 6.158315658569336 || Accuracy: %0.2317187488079071 || LR: 0.004000000189989805\n",
            "[118500] Loss: 10.159194946289062 || Reg Loss: 6.161404609680176 || Accuracy: %0.23720312118530273 || LR: 0.004000000189989805\n",
            "[119000] Loss: 10.225675582885742 || Reg Loss: 6.16617488861084 || Accuracy: %0.2291875034570694 || LR: 0.004000000189989805\n",
            "[119500] Loss: 10.019031524658203 || Reg Loss: 6.169334411621094 || Accuracy: %0.24124999344348907 || LR: 0.004000000189989805\n",
            "[120000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[120000] Loss: 9.853899955749512 || Reg Loss: 6.167944431304932 || Accuracy: %0.2448437511920929 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:09<00:00,  1.75it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9901666666666668 || Best Threshold --> 1.1500000000000001\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_120000.h5\n",
            "2024-08-04 21:24:12.812390: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 21:24:12.812448: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_120000.tflite\n",
            "2024-08-04 21:25:24.844193: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 21:25:24.844255: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_120000.tflite\n",
            "[*] Final model saved\n",
            "[120500] Loss: 9.990955352783203 || Reg Loss: 6.169317245483398 || Accuracy: %0.2406875044107437 || LR: 0.004000000189989805\n",
            "[121000] Loss: 9.921560287475586 || Reg Loss: 6.171544075012207 || Accuracy: %0.24109375476837158 || LR: 0.004000000189989805\n",
            "[121500] Loss: 9.984265327453613 || Reg Loss: 6.175078868865967 || Accuracy: %0.23862500488758087 || LR: 0.004000000189989805\n",
            "[122000] Loss: 10.205513000488281 || Reg Loss: 6.181699752807617 || Accuracy: %0.23212499916553497 || LR: 0.004000000189989805\n",
            "[122500] Loss: 10.415994644165039 || Reg Loss: 6.186518669128418 || Accuracy: %0.23084375262260437 || LR: 0.004000000189989805\n",
            "[123000] Loss: 10.270992279052734 || Reg Loss: 6.188106060028076 || Accuracy: %0.2329687476158142 || LR: 0.004000000189989805\n",
            "[123500] Loss: 10.119743347167969 || Reg Loss: 6.189455032348633 || Accuracy: %0.23926562070846558 || LR: 0.004000000189989805\n",
            "[124000] Loss: 9.862130165100098 || Reg Loss: 6.189108848571777 || Accuracy: %0.24659374356269836 || LR: 0.004000000189989805\n",
            "[124500] Loss: 9.881129264831543 || Reg Loss: 6.188473701477051 || Accuracy: %0.24712499976158142 || LR: 0.004000000189989805\n",
            "[125000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[125000] Loss: 10.137360572814941 || Reg Loss: 6.1913886070251465 || Accuracy: %0.23846875131130219 || LR: 0.004000000189989805\n",
            "[125500] Loss: 10.064666748046875 || Reg Loss: 6.194161891937256 || Accuracy: %0.23310936987400055 || LR: 0.004000000189989805\n",
            "[126000] Loss: 10.191145896911621 || Reg Loss: 6.20033073425293 || Accuracy: %0.2297656238079071 || LR: 0.004000000189989805\n",
            "[126500] Loss: 9.990964889526367 || Reg Loss: 6.202378273010254 || Accuracy: %0.23404687643051147 || LR: 0.004000000189989805\n",
            "[127000] Loss: 10.014609336853027 || Reg Loss: 6.206860542297363 || Accuracy: %0.24199999868869781 || LR: 0.004000000189989805\n",
            "[127500] Loss: 10.279753684997559 || Reg Loss: 6.213099956512451 || Accuracy: %0.2270781248807907 || LR: 0.004000000189989805\n",
            "[128000] Loss: 10.233098030090332 || Reg Loss: 6.218841552734375 || Accuracy: %0.230531245470047 || LR: 0.004000000189989805\n",
            "[128500] Loss: 10.380194664001465 || Reg Loss: 6.223128795623779 || Accuracy: %0.22584375739097595 || LR: 0.004000000189989805\n",
            "[129000] Loss: 10.109949111938477 || Reg Loss: 6.223631858825684 || Accuracy: %0.23465624451637268 || LR: 0.004000000189989805\n",
            "[129500] Loss: 10.011214256286621 || Reg Loss: 6.228250980377197 || Accuracy: %0.24296875298023224 || LR: 0.004000000189989805\n",
            "[130000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[130000] Loss: 9.965214729309082 || Reg Loss: 6.2323994636535645 || Accuracy: %0.24290624260902405 || LR: 0.004000000189989805\n",
            "-----------------------------------\n",
            "100% 750/750 [07:02<00:00,  1.77it/s]\n",
            "[*] Results on LFW, Accuracy --> 0.9918333333333333 || Best Threshold --> 1.1400000000000001\n",
            "-----------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[*] Final feature extractor saved to /gdrive/My Drive/Arcface/model_130000.h5\n",
            "2024-08-04 21:57:26.248583: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 21:57:26.248634: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 398, Total Ops 713, % non-converted = 55.82 %\n",
            " * 398 ARITH ops\n",
            "\n",
            "- arith.constant:  398 occurrences  (f32: 397, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "[*] Model saved in TFLite format to /gdrive/My Drive/Arcface/model_130000.tflite\n",
            "2024-08-04 21:58:37.844408: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2024-08-04 21:58:37.844462: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 266, Total Ops 713, % non-converted = 37.31 %\n",
            " * 266 ARITH ops\n",
            "\n",
            "- arith.constant:  266 occurrences  (f32: 265, i32: 1)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 21)\n",
            "  (f32: 23)\n",
            "  (f32: 132)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 131)\n",
            "  (uq_8: 132)\n",
            "[*] Quantized model saved in TFLite format to /gdrive/My Drive/Arcface/model_quantized_130000.tflite\n",
            "[*] Final model saved\n",
            "[130500] Loss: 9.900145530700684 || Reg Loss: 6.233894348144531 || Accuracy: %0.24503125250339508 || LR: 0.004000000189989805\n",
            "[131000] Loss: 9.87141227722168 || Reg Loss: 6.235443115234375 || Accuracy: %0.24124999344348907 || LR: 0.004000000189989805\n",
            "[131500] Loss: 9.980229377746582 || Reg Loss: 6.237734794616699 || Accuracy: %0.2409375011920929 || LR: 0.004000000189989805\n",
            "[132000] Loss: 9.881507873535156 || Reg Loss: 6.240859508514404 || Accuracy: %0.23829688131809235 || LR: 0.004000000189989805\n",
            "[132500] Loss: 10.064916610717773 || Reg Loss: 6.246123313903809 || Accuracy: %0.23929686844348907 || LR: 0.004000000189989805\n",
            "[133000] Loss: 10.047890663146973 || Reg Loss: 6.250681400299072 || Accuracy: %0.23676562309265137 || LR: 0.004000000189989805\n",
            "[133500] Loss: 9.763411521911621 || Reg Loss: 6.251236438751221 || Accuracy: %0.2448125034570694 || LR: 0.004000000189989805\n",
            "[134000] Loss: 9.669551849365234 || Reg Loss: 6.252377510070801 || Accuracy: %0.2500937581062317 || LR: 0.004000000189989805\n",
            "[134500] Loss: 9.566023826599121 || Reg Loss: 6.249237060546875 || Accuracy: %0.25303125381469727 || LR: 0.004000000189989805\n",
            "[135000] Model saved to /gdrive/My Drive/Arcface/ArcFaceModel/model.tf\n",
            "[135000] Loss: 9.535292625427246 || Reg Loss: 6.248106956481934 || Accuracy: %0.2555156350135803 || LR: 0.004000000189989805\n",
            "[135500] Loss: 9.618983268737793 || Reg Loss: 6.248747825622559 || Accuracy: %0.24745312333106995 || LR: 0.004000000189989805\n",
            "[136000] Loss: 9.707513809204102 || Reg Loss: 6.250921249389648 || Accuracy: %0.24946874380111694 || LR: 0.004000000189989805\n"
          ]
        }
      ],
      "source": [
        "!python train_classifier.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYEJHoEuX32N",
        "outputId": "4cd753ae-8ba5-4ff8-f6c1-92c9dff3d4fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-05 07:36:51.257494: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-05 07:36:51.257563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-05 07:36:51.258825: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-05 07:36:52.325770: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/lambda_layer.py:327: UserWarning: model_scripts.inception_resnet_v1 is not loaded, but a Lambda layer uses it. It may cause errors.\n",
            "  function = cls._parse_function_from_config(\n",
            "2024-08-05 07:36:55.237262: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 1s 816ms/step\n",
            "1/1 [==============================] - 0s 235ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 265ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "3/3 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "Distance --> 0.5660521388053894, Those images belong to same person.\n",
            "Figure(800x800)\n",
            "Figure(800x800)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference_tflite.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P7s3iSyeDUU",
        "outputId": "97c16c8e-9dfe-437c-fbaa-f6156fcee870"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-05 07:44:38.335284: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-05 07:44:38.335345: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-05 07:44:38.336628: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-05 07:44:39.451218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "2024-08-05 07:44:41.894136: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "1/1 [==============================] - 1s 834ms/step\n",
            "1/1 [==============================] - 0s 236ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "1/1 [==============================] - 0s 269ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "3/3 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "Distance --> 0.5661225318908691, Those images belong to same person.\n",
            "Figure(800x800)\n",
            "Figure(800x800)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assuming images are already face images. So face detector is removed\n",
        "# also \"data/t2_face.jpg\" and \"data/t4_face.jpg\" used\n",
        "!python inference_tflite_without_detector.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dP3a9J_eG7K",
        "outputId": "ee724524-7efa-490d-bc96-73cf0dc6be3c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-05 07:55:52.891502: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-05 07:55:52.891549: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-05 07:55:52.892803: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-05 07:55:53.911475: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "2024-08-05 07:55:56.008110: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Distance: 0.7586747407913208, Result: same person\n",
            "Figure(800x800)\n",
            "Figure(800x800)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}